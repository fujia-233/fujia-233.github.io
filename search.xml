<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Nginx</title>
      <link href="/2024/12/04/Nginx/"/>
      <url>/2024/12/04/Nginx/</url>
      
        <content type="html"><![CDATA[<h1 id="Nginx"><a href="#Nginx" class="headerlink" title="Nginx"></a><strong>Nginx</strong></h1><p>别名：engine x</p><p>百科：<em>Nginx</em> (engine x) 是一个高性能的<a href="https://baike.baidu.com/item/HTTP">HTTP</a>和<a href="https://baike.baidu.com/item/%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86/7793488">反向代理</a>web服务器，同时也提供了IMAP&#x2F;POP3&#x2F;SMTP服务。</p><p>主要功能：<strong>反向代理</strong>，<strong>负载均衡</strong>，<strong>动静分离</strong></p><p>1.项目刚刚上线的时候，并发量小，用户使用的少，所以在低并发的情况下，一个jar包启动应用就够了，然后内部tomcat返回内容给用户。</p><p>2.但是慢慢的，使用我们平台的用户越来越多了，并发量慢慢增大了，这时候一台服务器满足不了我们的需求了。</p><p>3.于是我们横向扩展，又增加了服务器。这个时候几个项目启动在不同的服务器上，用户要访问，就需要增加一个代理服务器了，通过代理服务器来帮我们转发和处理请求。</p><p>4.我们希望这个代理服务器可以帮助我们接收用户的请求，然后将用户的请求按照规则帮我们转发到不同的服务器节点之上。这个过程用户是无感知的，用户并不知道是哪个服务器返回的结果，我们还希望他可以按照服务器的性能提供不同的权重选择。保证最佳体验！所以我们使用了Nginx。</p><h2 id="一、正向代理和反向代理"><a href="#一、正向代理和反向代理" class="headerlink" title="一、正向代理和反向代理"></a>一、正向代理和反向代理</h2><p><strong>正向代理</strong>代理的对象是客户端，<strong>反向代理</strong>代理的对象是服务端</p><p><strong>正向代理：</strong></p><p>用户知道目标服务器地址，但由于网络限制等原因，无法直接访问。这时候需要先连接代理服务器，然后再由代理服务器访问目标服务器。</p><p>例如：由于防火墙的原因，我们并不能直接访问谷歌，那么我们可以借助VPN来实现。正向代理“代理”的是客户端，而且客户端是知道目标的，而目标是不知道客户端是通过VPN访问的。</p><p><img src="/../images/postsImg/Nginx/v2-c8ac111c267ae0745f984e326ef0c47f_720w.jpg" alt="img"></p><p><strong>反向代理：</strong></p><p>当我们在外网访问百度的时候，其实会进行一个转发，代理到内网去，这就是所谓的反向代理，即反向代理“代理”的是服务器端，而且这一个过程对于客户端而言是透明的。</p><p><img src="/../images/postsImg/Nginx/v2-4787a512240b238ebf928cd0651e1d99_720w.jpg" alt="img"></p><h2 id="二、负载均衡"><a href="#二、负载均衡" class="headerlink" title="二、负载均衡"></a>二、负载均衡</h2><p>Nginx提供的负载均衡策略有2种：内置策略和扩展策略。内置策略为轮询，加权轮询，Ip hash。</p><p>轮询<br><img src="/../images/postsImg/Nginx/kuangstudy4d33dfac-1949-4b2d-abb8-fe0b6e65b8dc.png" alt="img"><br>加权轮询<br><img src="/../images/postsImg/Nginx/kuangstudyb1e3e440-4159-4259-a174-528b56cb04b2.png" alt="img"><br>iphash对客户端请求的ip进行hash操作，然后根据hash结果将同一个客户端ip的请求分发给同一台服务器进行处理，可以解决session不共享的问题。<br><img src="/../images/postsImg/Nginx/kuangstudy64acb9a3-cd1a-4c0e-a1fa-9b220046a95a.png" alt="img"></p><h2 id="三、动静分离"><a href="#三、动静分离" class="headerlink" title="三、动静分离"></a>三、动静分离</h2><p>动静分离，在我们的软件开发中，有些请求是需要后台处理的，有些请求是不需要经过后台处理的（如：css、html、jpg、js等等文件），这些不需要经过后台处理的文件称为静态文件。让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后，我们就可以根据静态资源的特点将其做缓存操作。提高资源响应的速度。</p><p><img src="/../images/postsImg/Nginx/kuangstudyedb1bbd6-e530-4aba-8fde-68658a10e73f.png" alt="img"></p><h2 id="四、使用"><a href="#四、使用" class="headerlink" title="四、使用"></a>四、使用</h2><h3 id="1-下载"><a href="#1-下载" class="headerlink" title="1.下载"></a>1.下载</h3><p>官网下载<a href="http://nginx.org/en/download.html%EF%BC%9A">http://nginx.org/en/download.html：</a></p><p><img src="/../images/postsImg/Nginx/image-20220109174326668.png" alt="img"></p><p>下载解压后目录：</p><p><img src="/../images/postsImg/Nginx/image-20220109174115257.png" alt="image-20220109174115257"></p><h3 id="2-启动"><a href="#2-启动" class="headerlink" title="2.启动"></a>2.启动</h3><p>有很多种方法启动nginx</p><p>(1)直接双击nginx.exe，双击后一个黑色的弹窗一闪而过</p><p>(2)打开cmd命令窗口，切换到nginx解压目录下，输入命令 <code>nginx.exe</code> ，回车即可</p><h3 id="3-检查nginx是否启动成功"><a href="#3-检查nginx是否启动成功" class="headerlink" title="3.检查nginx是否启动成功"></a>3.检查nginx是否启动成功</h3><p>直接在浏览器地址栏输入网址 <a href="http://localhost/">http://localhost:80</a> 回车，出现以下页面说明启动成功！</p><p><img src="/../images/postsImg/Nginx/kuangstudya21688c8-159e-4caa-8e65-3dc056b6b78e.png" alt="img"></p><h3 id="4-配置监听"><a href="#4-配置监听" class="headerlink" title="4.配置监听"></a>4.配置监听</h3><p>nginx的配置文件是conf目录下的nginx.conf，默认配置的nginx监听的端口为80，如果80端口被占用可以修改为未被占用的端口即可。</p><p><img src="/../images/postsImg/Nginx/kuangstudyf23105c4-b0b2-4e22-a1bf-b8098f40c144.png" alt="img"></p><p>当我们修改了nginx的配置文件nginx.conf 时，不需要关闭nginx后重新启动nginx，只需要执行命令 <strong>nginx -s reload</strong> 即可让改动生效</p><h3 id="5-简单使用"><a href="#5-简单使用" class="headerlink" title="5.简单使用"></a>5.简单使用</h3><p>在conf目录下的nginx.conf文件里配置Nginx</p><p>主要配置：</p><p><img src="/../images/postsImg/Nginx/image-20220109182109337.png" alt="image-20220109182109337"></p><p>反向代理和负载均衡配置</p><p><img src="/../images/postsImg/Nginx/image-20220109182933006.png" alt="image-20220109182933006"></p><p>配置完后执行命令 <strong>nginx -s reload</strong> 即可让改动生效</p><h3 id="6-关闭nginx"><a href="#6-关闭nginx" class="headerlink" title="6.关闭nginx"></a>6.关闭nginx</h3><p>如果使用cmd命令窗口启动nginx， 关闭cmd窗口是不能结束nginx进程的，可使用两种方法关闭nginx</p><p>(1)输入nginx命令 <code>nginx -s stop</code>(快速停止nginx) 或 <code>nginx -s quit</code>(完整有序的停止nginx)</p><p>(2)使用taskkill <code>taskkill /f /t /im nginx.exe</code></p><ol><li><code>taskkill是用来终止进程的，</code></li><li><code>/f是强制终止 .</code></li><li><code>/t终止指定的进程和任何由此启动的子进程。</code></li><li><code>/im示指定的进程名称 .</code></li></ol>]]></content>
      
      
      <categories>
          
          <category> Nginx </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
            <tag> Java </tag>
            
            <tag> Nginx </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>决策树分类算法研究</title>
      <link href="/2024/06/28/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/"/>
      <url>/2024/06/28/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="基于人工智能的决策树分类算法研究"><a href="#基于人工智能的决策树分类算法研究" class="headerlink" title="基于人工智能的决策树分类算法研究"></a><strong>基于人工智能的决策树分类算法研究</strong></h1><h2 id="一、实验目的"><a href="#一、实验目的" class="headerlink" title="一、实验目的"></a>一、实验目的</h2><p>​决策树归纳是最简单但最成功的学习算法之一。决策树（DT）由内部和外部节点以及组成节点之间的互连称为树的分支。内部节点是一个决策单元根据不同决定下一个要访问的子节点相关变量的可能值。相比之下，外部节点也称为树叶节点，是分支的终止节点。它没有任何子节点并且是与用来描述给定数据的类标签相关联。决策树是树中的一组规则结构，其中的每个分支都可以被解释为沿着这个分支访问的节点与之相关的决策规则。</p><p>​决策树通过从树根到树叶节点对树进行排序来对实例进行分类。这个树形结构的分类器将数据集的输入空间递归分割互相排斥的空间。按照这种结构，每个训练数据被标识为属于某个子空间，它被分配了一个标签，一个值或一个动作表征其数据点。决策树机制具有良好的透明度我们可以很容易地遵循树状结构来解释如何做出决定。因此，当我们澄清有条件的规则特征时，可解释性就会增强。</p><p>​随机变量的熵通过生成的平均信息量观察它的价值。考虑用硬币掷硬币的随机实验正面的概率等于0.9，因此P（Head）&#x3D; 0.9，P（Tail）&#x3D; 0.1。这可提供比P（Head）&#x3D; 0.5和P（Tail）&#x3D; 0.5的情况更多的信息。</p><p>​熵通常用于评估物理学中的随机性，其中熵值较大表明该过程非常随机。决策树是根据每个属性的信息内容进行启发式指导的。熵作为分类的手段用于评估每个属性的信息。</p><p>​我们也可以说熵是一个样本集合中的杂质度：熵越大，数据越不纯。基于熵，信息增益（IG）通常被用于衡量类间区分的有效性。</p><h2 id="二、决策树分类算法实现"><a href="#二、决策树分类算法实现" class="headerlink" title="二、决策树分类算法实现"></a>二、决策树分类算法实现</h2><h3 id="2-1模型构建"><a href="#2-1模型构建" class="headerlink" title="2.1模型构建"></a>2.1模型构建</h3><ol><li>定义决策树的类。其中包含分类特征，分类界值，第一类，第二类，第一类的熵，第二类的熵。每进行一次分类后，均保存为这样一个数据结构；</li><li>加载鸢尾花数据集。数据集包含5列，前四列分别是分类的特征：“花萼长度”, “花萼宽度”, “花瓣长度”, “花瓣宽度”，最后一列是样本的标签，即该一行的鸢尾花属于什么类别，此处共有3个类别；</li><li>构建决策树。主要是利用递归的思想，先找到第一层的最佳分类点，即找到最佳分类属性与分类值，分割数据集后再继续进行下一层的最佳分类点搜寻，直到划分后的数据不能再划分为止。其中，寻找最佳分类点子过程为，对每一个属性，每一个值当做分类点，计算此时的信息熵与信息增益，找到信息增益最大的分类属性与分类值，再划分数据集，保存构建的决策树类。</li></ol><h3 id="2-2决策树的可视化"><a href="#2-2决策树的可视化" class="headerlink" title="2.2决策树的可视化"></a>2.2决策树的可视化</h3><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/clip_image002.png" alt="20200717013828866"></p><h3 id="2-3结果分析"><a href="#2-3结果分析" class="headerlink" title="2.3结果分析"></a>2.3结果分析</h3><p>​程序运行结果：输出决策树每一次分类的分类属性以及分类值，每一次分类之后的熵，到不能再分类时该类的类别以及该类别的数量。</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/clip_image001.png" alt="img"></p><p>​根据输出结果，画出树状的分类结果图如图3，分类到最后三种类别，分别用三种颜色来表示，（山鸢尾：红色；变色鸢尾：绿色；维吉尼亚鸢尾：蓝色）可以看到，在第一次分类时，即通过花瓣宽度将第一类山鸢尾分出来，后面最多5次分类即可将三种类别全部分割出来。</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/clip_image002.jpg" alt="20200717014022158"></p><h2 id="三、对比综合实验"><a href="#三、对比综合实验" class="headerlink" title="三、对比综合实验"></a>三、对比综合实验</h2><p>​为了更好地对比实验，选用经典二分类问题，即选择scikit-learn中breast_cancer（乳腺癌）数据集进行实验分析。</p><p>​数据描述：该分类数据集包含569条数据，30个属性，均为连续属性，目标是对该数据集进行二分类问题，即判断属于’malignant’（恶性）, ‘benign’（良性）中的哪一类。</p><p>​本程序将569份数据按照7:3比例进行划分，其中400份数据作为训练集，169份数据作为测试集。</p><h3 id="3-1决策树深度对模型精度影响"><a href="#3-1决策树深度对模型精度影响" class="headerlink" title="3.1决策树深度对模型精度影响"></a>3.1决策树深度对模型精度影响</h3><p>​为了探索决策树的大小和模型准确率之间的影响和联系，方便起见，调用sklearn中的决策树分类工具包(DecisionTreeClassifier)，设置决策树最深深度这一参数(max_depth)分别为4,5,6,7,8进行实验，在训练集上(400份数据)训练，在测试集(169份数据)进行测试。同时记录训练模型所用的时间，输出最终生成树的大小，输出模型的准确率，并且输出分类模型的评价(包含precision, recall, f1-score, support)，输出模型的混淆矩阵。</p><p>​在测试完上述四个参数后，汇总结果，输出决策树的平均耗时，并且输出最优决策树模型(以准确率作为评判标准)，最大深度，以及相应的准确率，最终，得到结果如下图所示：</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/clip_image002-17337956989511.jpg" alt="img"></p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/clip_image002-17337957102882.jpg" alt="img"></p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/clip_image002-17337957156553.jpg" alt="img"></p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/clip_image001-17337957208024.png" alt="4"></p><p>​由以上输出结果表明，决策树最大深度为6时，准确率为0.905，此时，训练决策树共耗时0.006s，决策树叶子结点为15，相应的f1-score等评价指标也较高，此时模型结果达到最优。</p><p>​由于本数据集数据量较小，训练较快，训练的耗时均比较小，没有比较的意义；另外，随着最深深度的增加，模型的叶子结点数量也在增加，决策树变得更加庞大和细致，但是，在测试集上的表现并没有随着决策树增大而效果变好，可能是出现了对训练集过拟合的情况，因此，在选择决策树最深深度时，需要根据实际情况，选择适当的深度。</p><h3 id="3-2决策树特征选择标准对模型精度影响"><a href="#3-2决策树特征选择标准对模型精度影响" class="headerlink" title="3.2决策树特征选择标准对模型精度影响"></a>3.2决策树特征选择标准对模型精度影响</h3><p>​同上述探究决策树大小对模型精度影响的研究方法，对参数特征选择标准criterion进行设置，可以使用”gini”或者”entropy”，前者代表基尼系数，后者代表信息增益。默认使用的是基尼系数”gini”，即CART算法，但最基础的决策树分类算法是基于ID3, C4.5的最优特征选择方法，无论哪种方式是都可以适用于本数据集类型的连续变量。进行选择后，输出结果如下：</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/clip_image002-17337957499985.jpg" alt="img"></p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/clip_image001-17337957546506.png" alt="6"></p><p>​通过分析，选择信息增益为特征选择标准，在小数据集上效果较好，而大数据集上，虽然在对训练样本集的学习中可以尽可能多地挖掘信息，但其生成的决策树分支较大，规模较大，为了简化决策树的规模，提高生成决策树的效率，就出现了根据GINI系数来选择测试属性的决策树算法CART。由于本数据集数据量不大，仅有400个作为训练样本，因此相对来说，选用信息增益为特征选择标准对模型表现结果较好。</p><h3 id="3-3决策树叶子结点最小样本数对模型精度的影响"><a href="#3-3决策树叶子结点最小样本数对模型精度的影响" class="headerlink" title="3.3决策树叶子结点最小样本数对模型精度的影响"></a>3.3决策树叶子结点最小样本数对模型精度的影响</h3><p>​探究决策树叶子结点最小样本数对模型精度影响，本质上是在探究“剪枝”策略对决策树的影响，沿用上述两种对比试验的研究方法，对参数特征叶子结点最小样本数min_samples_leaf进行设置，在这里设置参数分别为1,2,3,4,5，输出结果如下：</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/clip_image002-17337957861017.jpg" alt="img"></p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/clip_image002-17337957902068.jpg" alt="img"></p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/clip_image002-17337957949709.jpg" alt="img"></p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/clip_image002-173379579845210.png" alt="11"></p><p>​由以上输出结果表明，决策树叶子结点最小样本数为3时，准确率为0.905，此时，训练决策树共耗时0.005s，决策树叶子结点为14，相应的f1-score等评价指标也较高，此时模型结果达到最优。</p><p>​由实验结果可知，决策树叶子结点最小样本数越大，一般情况下决策树分支会减小，决策树整体变小，有一定的抵抗对训练数据过拟合的能力，但随着叶子结点最小样本数大到一定程度，模型的准确性又会难以保证，本次实验得到当叶子结点最小样本数为3的时候，模型准确度最高，因此，在选择决策树叶子结点最小样本数时，需要根据实际情况，选择适当的值，即通过“剪枝”的思想，控制决策树的大小，提升模型对于测试集的精度。</p><h3 id="3-4最优决策树相关分析"><a href="#3-4最优决策树相关分析" class="headerlink" title="3.4最优决策树相关分析"></a>3.4最优决策树相关分析</h3><p>​通过上述的各个参数的调整与研究，总结测试，得出结论，在本项目中，选用决策树的参数为criterion&#x3D;‘entropy’,max_depth&#x3D;15,min_samples_leaf&#x3D;3，输出结果如下：</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/clip_image001-173379582952011.png" alt="img"></p><p>​由以上输出结果表明，此时模型的准确率达到0.935达到最高，此时，训练决策树共耗时0.0132s，决策树深度为5，叶子结点为12，相应的f1-score等评价指标也较高，此时模型结果达到最优。</p><p>​接下来，调用graphviz和pydotplus工具包，设置好参数路径，可视化决策树：</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/clip_image002-173379584403812.jpg" alt="img"></p><p>​蓝色代表分类为正常’benign’，橙色代表分类为患病’malignant’，颜色越深表示此类样本中只含有这一类，越浅表示分类界限不明确。综上，决策树可以较好地完成该二分类问题。</p><p>​接下来，为了观察决策树的分类效果，根据决策树的分类过程对采用的30个特征做重要度排序：</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/clip_image002-173379586509713.jpg" alt="img"></p><p>​可以看到’worst perimeter’和’wort concave points’这两个特征的重要度最高，以这两个特征分别为横轴和纵轴，绘制样本的散点图，并且根据排序结果进行分类，有以下结果：</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/clip_image001-173379587735314.png" alt="img"></p><p>​由上图，可以看到黄颜色的样本和紫颜色的样本成功被分为两类，分类界限也比较明显，决策树分类效果比较好。</p><h3 id="3-5贝叶斯网络分类"><a href="#3-5贝叶斯网络分类" class="headerlink" title="3.5贝叶斯网络分类"></a>3.5贝叶斯网络分类</h3><p>​为了对比贝叶斯网络与决策树分类的结果，基于相同的scikit-learn中breast_cancer（乳腺癌）数据集，调用sklearn.naive_bayes中的GaussianNB分类模型进行实验，有以下输出结果：</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6/clip_image001-173379591777415.png" alt="img"></p><p>​由上述输出结果可以看到准确率达到了0.964，没有参数的调整，效果优于决策树分类的最佳结果0.935。说明在该问题中，样本数量不是很庞大，贝叶斯网络效果更佳，另外，由于人口中不会大量发生癌症，样本的不均衡性，也导致决策树算法的分类性能降低。综上所述，在本项目中适合采用朴素贝叶斯网络进行分类。</p><h3 id="3-6对比综合实验结论"><a href="#3-6对比综合实验结论" class="headerlink" title="3.6对比综合实验结论"></a>3.6对比综合实验结论</h3><p>​与朴素贝叶斯分类相比，决策树学习的优点和缺点。</p><p>​最优决策树的相关分析，贝叶斯网络的分类结果可以看到，针对同样的数据集，两种方法均能够较好地完成该二分类的任务，但是就具体问题而言，贝叶斯网络的效果更好，原因可能是因为数据样本比较少，样本不均衡等原因。通过实验体会和查阅资料，总结：</p><p><strong>决策树的优点：</strong></p><ol><li>决策树易于理解和解释；</li><li>能够同时处理数据型和常规型属性；</li><li>在相对短的时间内能够对大型数据源做出可行且效果良好的结果；</li><li>比较适合处理有缺失属性的样本；</li><li>能够处理不相关的特征；</li></ol><p><strong>决策树的缺点：</strong></p><ol><li>对于那些各类别样本数量不一致的数据，在决策树中，进行属性划分时，不同的判定准则会带来不同的属性选择倾向；</li><li>决策树处理缺失数据时的困难；</li><li>过度拟合问题的出现；</li><li>忽略数据集中属性之间的相关性。</li></ol><h2 id="四、总结"><a href="#四、总结" class="headerlink" title="四、总结"></a>四、总结</h2><p>​通过此次决策树的学习和代码的运行，我掌握了决策树基础算法的逻辑，和如何解析数据使得数据符合决策树算法输入。决策树的算法不需要调整过多的参数，同时算法的可解释性也非常简单直观。但决策树的缺点也同样在此次实验中暴露出来了，当决策属性过多，整个决策树的算法的开销将会膨胀到非常大。同时这样扩展出来的数据集也容易达到过拟合的状态。于是学者之后还研发出来了预剪枝，和后剪枝等技术，以及使用随机森林算法来解决这些相关问题。</p>]]></content>
      
      
      <categories>
          
          <category> 学习日常 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 作业 </tag>
            
            <tag> 决策树 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>斑马问题实验</title>
      <link href="/2024/06/15/%E6%96%91%E9%A9%AC%E9%97%AE%E9%A2%98%E5%AE%9E%E9%AA%8C/"/>
      <url>/2024/06/15/%E6%96%91%E9%A9%AC%E9%97%AE%E9%A2%98%E5%AE%9E%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<h1 id="实验五-斑马问题实验"><a href="#实验五-斑马问题实验" class="headerlink" title="实验五 斑马问题实验"></a><strong>实验五 斑马问题实验</strong></h1><h2 id="一、实验背景"><a href="#一、实验背景" class="headerlink" title="一、实验背景"></a>一、实验背景</h2><ol><li>在本次实验中，我们学习将 Python 应用于逻辑编程，并尝试自主撰写逻辑规则解决斑马问题。</li><li>本章实验的主要目的是掌握逻辑与推理相关基础知识点，熟悉python编程。</li><li>能够依据给定的事实以及规则编写代码，解决逻辑约束问题。</li></ol><h2 id="二、实验简介"><a href="#二、实验简介" class="headerlink" title="二、实验简介"></a>二、实验简介</h2><p>​5个不同国家且工作各不相同的人分别住在一条街上的5所房子里，每所房子的颜色不同，每个人都有自己养的不同宠物，喜欢喝不同的饮料。</p><p>​根据以下提示，你能告诉我哪所房子里的人养斑马，哪所房子里的人喜欢喝矿泉水吗？</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">英国人住在红色的房子里</span><br><span class="line">西班牙人养了一条狗</span><br><span class="line">日本人是一个油漆工</span><br><span class="line">意大利人喜欢喝茶</span><br><span class="line">挪威人住在左边的第一个房子里</span><br><span class="line">绿房子在白房子的左边</span><br><span class="line">摄影师养了一只蜗牛</span><br><span class="line">外交官住在黄房子里</span><br><span class="line">中间那个房子的人喜欢喝牛奶</span><br><span class="line">喜欢喝咖啡的人住在绿房子里</span><br><span class="line">挪威人住在蓝色的房子旁边</span><br><span class="line">小提琴家喜欢喝橘子汁</span><br><span class="line">养狐狸的人所住的房子与医生的房子相邻</span><br><span class="line">养马的人所住的房子与外交官的房子相邻</span><br></pre></td></tr></table></figure><p><strong>存在的条件汇总：</strong></p><p>国籍：英国、西班牙、日本、意大利、挪威</p><p>房子颜色：红色、绿色、白色、蓝色、黄色</p><p>工作：油漆工、摄影师、外交官、小提琴家、医生</p><p>宠物：狗、蜗牛、狐狸、马、斑马</p><p>饮料：茶、牛奶、咖啡、橘子汁、矿泉水</p><h2 id="三、实验步骤"><a href="#三、实验步骤" class="headerlink" title="三、实验步骤"></a>三、实验步骤</h2><p>​使用python逻辑编程kanren库，将题目中的条件形式化为一个个的表达式，加入约束到kanren的一个集合中，然后利用kanren内置函数run求解即可。</p><p>​根据已知条件的性质将条件分为三组：</p><p><strong>1.第一组为属于一个实体的属性间的关系，有如下几个：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>英国人住在红房子里</span><br><span class="line"><span class="number">2.</span>西班牙人养了一条狗</span><br><span class="line"><span class="number">3.</span>日本人是一个油漆工</span><br><span class="line"><span class="number">4.</span>意大利人喝茶。</span><br><span class="line"><span class="number">7.</span>摄影师养了一只蜗牛</span><br><span class="line"><span class="number">8.</span>外交官住在黄房子里</span><br><span class="line"><span class="number">10.</span>喜欢喝咖啡的人住在绿房子里</span><br><span class="line"><span class="number">12.</span>小提琴家喜欢喝橘子汁</span><br></pre></td></tr></table></figure><p>​kanren中加入一个逻辑表达式的方法：</p><ul><li>首先用houses &#x3D; var() 创建一个逻辑变量houses。</li><li>代码中的var表示任意变量。5个var()分别代表国籍、工作、饮料、宠物、屋子颜色</li><li>然后利用membero函数表示一个约束, membero(item, houses) 表示 item是houses集合中的一个成员。如：</li></ul><p><img src="/../images/postsImg/%E6%96%91%E9%A9%AC%E9%97%AE%E9%A2%98%E5%AE%9E%E9%AA%8C/image-20241209152747529.png" alt="image-20241209152747529"></p><p><strong>2.第二组为表示实体在整体中的确切属性的条件</strong></p><p>​添加方法为eq函数，eq(a,b) 表示ab相等。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">5.</span>挪威人住在左边的第一个房子里</span><br><span class="line"><span class="number">9.</span>中间房子的人喜欢喝牛奶</span><br></pre></td></tr></table></figure><p><img src="/../images/postsImg/%E6%96%91%E9%A9%AC%E9%97%AE%E9%A2%98%E5%AE%9E%E9%AA%8C/image-20241209153132087.png" alt="image-20241209153132087"></p><p><strong>3.第三组为描述两个实体间的不确定的(模糊的)关系的表达式</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">6.</span>绿房子在白房子的左边</span><br><span class="line"><span class="number">11.</span>挪威人住在蓝色的房子旁边</span><br><span class="line"><span class="number">13.</span>养狐狸的人所住的房子与医生的房子相邻</span><br><span class="line"><span class="number">14.</span>养马的人所住的房子与外交官的房子相邻</span><br></pre></td></tr></table></figure><p>​在kanren中没有能够直接描述以上关系的函数，所以需要人为定义</p><p>​实现的函数：</p><p>​left(q,p,x)表示在关系集合x中q在p的左边</p><p>​next(q,p,x)表示在关系集合x中q和p相邻</p><p><img src="/../images/postsImg/%E6%96%91%E9%A9%AC%E9%97%AE%E9%A2%98%E5%AE%9E%E9%AA%8C/image-20241209153241032.png" alt="image-20241209153241032"></p><p>​借助以上两个函数就可以将对应的四个逻辑表达式表达出来了</p><p><img src="/../images/postsImg/%E6%96%91%E9%A9%AC%E9%97%AE%E9%A2%98%E5%AE%9E%E9%AA%8C/image-20241209153258415.png" alt="image-20241209153258415"></p><p>​到这里所有的表达式都已经表达完毕，已经可以利用run运行了，但是此时的表达式中还没有斑马和矿泉水这两个变量，程序处理到他们时会用符号代替，下面添加这两个变量进入表达式：</p><p><img src="/../images/postsImg/%E6%96%91%E9%A9%AC%E9%97%AE%E9%A2%98%E5%AE%9E%E9%AA%8C/image-20241209153316952.png" alt="image-20241209153316952"></p><p>最后运行规则代码：</p><p><img src="/../images/postsImg/%E6%96%91%E9%A9%AC%E9%97%AE%E9%A2%98%E5%AE%9E%E9%AA%8C/image-20241209153325516.png" alt="image-20241209153325516"></p><h2 id="四、实验代码总结"><a href="#四、实验代码总结" class="headerlink" title="四、实验代码总结"></a>四、实验代码总结</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> kanren <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> kanren.core <span class="keyword">import</span> lall</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">left</span>(<span class="params">q, p, <span class="built_in">list</span></span>):</span><br><span class="line">    <span class="keyword">return</span> membero((q, p), <span class="built_in">zip</span>(<span class="built_in">list</span>, <span class="built_in">list</span>[<span class="number">1</span>:]))</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">right_of</span>(<span class="params">q, p, <span class="built_in">list</span></span>):</span><br><span class="line">    <span class="keyword">return</span> left(p, q, <span class="built_in">list</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">next_to</span>(<span class="params">q, p, <span class="built_in">list</span></span>):</span><br><span class="line">    <span class="keyword">return</span> conde([left(q, p, <span class="built_in">list</span>)], [right_of(q, p, <span class="built_in">list</span>)])</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Agent</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.units = var()  <span class="comment"># 单个unit变量指代一座房子的信息(国家，工作，饮料，宠物，颜色)</span></span><br><span class="line">        <span class="comment"># 例如(&#x27;英国人&#x27;, &#x27;油漆工&#x27;, &#x27;茶&#x27;, &#x27;狗&#x27;, &#x27;红色&#x27;)即为正确格式，但不是本题答案</span></span><br><span class="line">        <span class="comment"># 请基于给定的逻辑提示求解五条正确的答案</span></span><br><span class="line">        <span class="variable language_">self</span>.rules_zebraproblem = <span class="literal">None</span>  <span class="comment"># 用lall包定义逻辑规则</span></span><br><span class="line">        <span class="variable language_">self</span>.solutions = <span class="literal">None</span>  <span class="comment"># 存储结果</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">define_rules</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.rules_zebraproblem = lall(</span><br><span class="line">            (eq, (var(), var(), var(), var(), var()), <span class="variable language_">self</span>.units),</span><br><span class="line">            <span class="comment"># self.units共包含五个unit成员，即每一个unit对应的var都指代一座房子(国家，工作，饮料，宠物，颜色)</span></span><br><span class="line">            <span class="comment"># 各个unit房子又包含五个成员属性: (国家，工作，饮料，宠物，颜色)</span></span><br><span class="line">            (membero, (<span class="string">&#x27;英国人&#x27;</span>, var(), var(), var(), <span class="string">&#x27;红色&#x27;</span>), <span class="variable language_">self</span>.units),</span><br><span class="line">            (membero, (<span class="string">&#x27;西班牙人&#x27;</span>, var(), var(), <span class="string">&#x27;狗&#x27;</span>, var()), <span class="variable language_">self</span>.units),</span><br><span class="line">            (membero, (<span class="string">&#x27;日本人&#x27;</span>, <span class="string">&#x27;油漆工&#x27;</span>, var(), var(), var()), <span class="variable language_">self</span>.units),</span><br><span class="line">            (membero, (<span class="string">&#x27;意大利人&#x27;</span>, var(), <span class="string">&#x27;茶&#x27;</span>, var(), var()), <span class="variable language_">self</span>.units),</span><br><span class="line">            (eq, ((<span class="string">&#x27;挪威人&#x27;</span>, var(), var(), var(), var()), var(), var(), var(), var()), <span class="variable language_">self</span>.units),</span><br><span class="line">            (right_of, (var(), var(), var(), var(), <span class="string">&#x27;绿色&#x27;</span>), (var(), var(), var(), var(), <span class="string">&#x27;白色&#x27;</span>),<span class="variable language_">self</span>.units),</span><br><span class="line">            (membero, (var(), <span class="string">&#x27;摄影师&#x27;</span>, var(), <span class="string">&#x27;蜗牛&#x27;</span>, var()), <span class="variable language_">self</span>.units),</span><br><span class="line">            (membero, (var(), <span class="string">&#x27;外交官&#x27;</span>, var(), var(), <span class="string">&#x27;黄色&#x27;</span>), <span class="variable language_">self</span>.units),</span><br><span class="line">            (eq, (var(), var(), (var(), var(), <span class="string">&#x27;牛奶&#x27;</span>, var(), var()), var(), var()), <span class="variable language_">self</span>.units),</span><br><span class="line">            (membero, (var(), var(), <span class="string">&#x27;咖啡&#x27;</span>, var(), <span class="string">&#x27;绿色&#x27;</span>), <span class="variable language_">self</span>.units),</span><br><span class="line">            (next_to, (<span class="string">&#x27;挪威人&#x27;</span>, var(), var(), var(), var()), (var(), var(), var(), var(), <span class="string">&#x27;蓝色&#x27;</span>), <span class="variable language_">self</span>.units),</span><br><span class="line">            (membero, (var(), <span class="string">&#x27;小提琴家&#x27;</span>, <span class="string">&#x27;橘子汁&#x27;</span>, var(), var()), <span class="variable language_">self</span>.units),</span><br><span class="line">            (next_to, (var(), var(), var(), <span class="string">&#x27;狐狸&#x27;</span>, var()), (var(), <span class="string">&#x27;医生&#x27;</span>, var(), var(), var()), <span class="variable language_">self</span>.units),</span><br><span class="line">            (next_to, (var(), var(), var(), <span class="string">&#x27;马&#x27;</span>, var()), (var(), <span class="string">&#x27;外交官&#x27;</span>, var(), var(), var()), <span class="variable language_">self</span>.units),</span><br><span class="line">            (membero, (var(), var(), <span class="string">&#x27;矿泉水&#x27;</span>, var(), var()), <span class="variable language_">self</span>.units),</span><br><span class="line">            (membero, (var(), var(), var(), <span class="string">&#x27;斑马&#x27;</span>, var()), <span class="variable language_">self</span>.units),</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">solve</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="variable language_">self</span>.define_rules()</span><br><span class="line">        <span class="variable language_">self</span>.solutions = run(<span class="number">0</span>, <span class="variable language_">self</span>.units, <span class="variable language_">self</span>.rules_zebraproblem)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.solutions</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    agent = Agent()</span><br><span class="line">    solutions = agent.solve()</span><br><span class="line">    <span class="comment"># 提取解释器的输出</span></span><br><span class="line">    output = [house <span class="keyword">for</span> house <span class="keyword">in</span> solutions[<span class="number">0</span>] <span class="keyword">if</span> <span class="string">&#x27;斑马&#x27;</span> <span class="keyword">in</span> house][<span class="number">0</span>][<span class="number">4</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;\n&#123;&#125;房子里的人养斑马&#x27;</span>.<span class="built_in">format</span>(output))</span><br><span class="line">    output = [house <span class="keyword">for</span> house <span class="keyword">in</span> solutions[<span class="number">0</span>] <span class="keyword">if</span> <span class="string">&#x27;矿泉水&#x27;</span> <span class="keyword">in</span> house][<span class="number">0</span>][<span class="number">4</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;&#123;&#125;房子里的人喜欢喝矿泉水&#x27;</span>.<span class="built_in">format</span>(output))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span>)</span><br><span class="line">    <span class="comment"># 解释器的输出结果展示</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> solutions[<span class="number">0</span>]:</span><br><span class="line">        <span class="built_in">print</span>(i)</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>运行代码截图</strong></p><p><img src="/../images/postsImg/%E6%96%91%E9%A9%AC%E9%97%AE%E9%A2%98%E5%AE%9E%E9%AA%8C/image-20241209153531897.png" alt="image-20241209153531897"></p><h2 id="五、实验总结"><a href="#五、实验总结" class="headerlink" title="五、实验总结"></a>五、实验总结</h2><p>​kanren库能够起到按照指定的表达式寻找满足表达式的值，下面是常用的kanren函数及其常用用法。</p><p>​eq(u,v)创建一个等式约束,表示变量a等于变量b,通常只是表达一个等式关系,返回值类型为一个逻辑表达式(函数)。</p><p>​var()用于创建逻辑变量,可以绑定到其他逻辑变量或常量。</p><p>​run(n,x,*goals)函数用于进行逻辑推理,返回指定个满足约束的变量绑定,返回值类型为python列表,列表的每个元素是字典。</p><p>​函数参数列表中n表示返回个数,x表示想要得到的逻辑变量,*goals指的是约束条件,约束条件可以有多个。</p>]]></content>
      
      
      <categories>
          
          <category> 学习日常 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 作业 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于循环神经网络的对联自动生成研究</title>
      <link href="/2024/06/09/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/"/>
      <url>/2024/06/09/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="基于循环神经网络的对联自动生成研究"><a href="#基于循环神经网络的对联自动生成研究" class="headerlink" title="基于循环神经网络的对联自动生成研究"></a><strong>基于循环神经网络的对联自动生成研究</strong></h1><h2 id="一、概述"><a href="#一、概述" class="headerlink" title="一、概述"></a>一、概述</h2><h3 id="1-1-选题背景及意义"><a href="#1-1-选题背景及意义" class="headerlink" title="1.1 选题背景及意义"></a>1.1 选题背景及意义</h3><p>​深度学习的概念最早由 Geoffrey Hinton 在 2006 年提出，其兴起于图像识别领域，在之后的很短时间内，深度学习技术广泛应用于机器学习的各个领域。在AI领域中，自然语言处理（Natural Language Processing，NLP）是一个占据着极其重要地位的子领域，它的发展可以使计算机理解和利用人类语言，使计算机达到认知智能，并逐步迈入感知智能[1]。NLP主要包括语言理解和语言生成两大部分。在语言生成范畴，诗歌和对联的自动生成是一个非常重要的分支，具有很强的前瞻性，一直是学术界研究的热点。</p><p>​在本文中，我们研究了对联的自动生成。我们提出了一个基于注意力的编码器-解码器模型来接收前从句，并在对联对中输出后续子句。给定任何指定的先行从句，我们学习单个字符的表示，以及从句中的组合，以及它们如何相互加强和约束。我们可以使用编码器解码器模型生成后续子句。在生成过程中，我们结合了注意力机制，以满足对联的特殊性。</p><p>​使用深度神经网络进行对联的生成研究是一项十分具有意义的工作，对联的自动生成，也是对自然语言处理领域中一个具体方面的深入探索和研究，对自然语言处理领域内的其他任务也有一定的借鉴参考价值；使用机器学习方法进行对联的创作，可以将其生成方法拓宽到其他场景下，促进中国传统文化的传播和发展。</p><h3 id="1-2-开发团队"><a href="#1-2-开发团队" class="headerlink" title="1.2 开发团队"></a>1.2 开发团队</h3><p><strong>组员：</strong></p><p>富佳、夏取明、王艺凡</p><p><strong>具体分工：</strong></p><p>富佳：组长；数据集整理，预处理；展示界面搭建；报告撰写</p><p>夏取明：模型搭建训练及模型预测； 参考文献收集</p><p>王艺凡：数据集搜集；参考文献收集；报告撰写</p><h3 id="1-3-开发环境"><a href="#1-3-开发环境" class="headerlink" title="1.3 开发环境"></a>1.3 开发环境</h3><table><thead><tr><th align="center">操作系统</th><th align="center">Windows</th></tr></thead><tbody><tr><td align="center">CPU型号</td><td align="center">AMD Ryzen 5 4600U with Radeon Graphics</td></tr><tr><td align="center">GPU型号</td><td align="center">AMD  Radeon(TM) Graphics</td></tr><tr><td align="center">内存</td><td align="center">16.0 GB</td></tr><tr><td align="center">主要工具</td><td align="center">Anaconda、Python3.6、paddle</td></tr></tbody></table><h3 id="1-4-国内外研究现状"><a href="#1-4-国内外研究现状" class="headerlink" title="1.4 国内外研究现状"></a>1.4 国内外研究现状</h3><p>​在以往的研究中很少有研究关注深度学习对中国对联的应用，但有类似的文本分类识别应用在机器翻译和欧美古典诗歌自动生成等方面。机器翻译的概念始于 1949 年，1954 年美国 Georgetown-IBM 实验室第一次完成英语和俄语间的机器翻译实验，证明了机器翻译的可行性。但是在后面一段时间内，由于速度慢、消耗计算资源高、准确性低等缺点，机器翻译的发展一度停滞。直到 20 世纪 80 年代，随着社会信息服务需求的扩大，机器翻译技术在处理大量文本翻译任务的优势逐渐凸显，机器翻译的研究开始复苏。</p><p>​古典诗歌自动生成的研究始于 1959 年，当时 Theo Lutz 通过计算机创作了第一首德语诗歌。从那时起，机器自动生成诗歌从简单的词堆栈方法开始，然后逐渐发展到现在的基于案例的推理方法以及其他新兴的方法，可分为基于模板的生成方法、基于遗传算法的方法、生成和测试方法以及基于案例的推理方法。很少有研究关注中国对联的产生。中国对联生成任务可以看作是两句诗生成的简化形式。给定诗的第一行，生成器应该相应地生成第二行，这与对联生成的过程类似。但是，对联生成和诗歌生成之间仍然存在一些差异。生成后续从句以匹配给定的前置从句的任务比生成一首诗的所有句子更明确。此外，并不是诗中所有的句子都需要遵循对联的约束。</p><p>​总的来说，国内基于seq2seq的对联自动生成系统在技术上已经取得了一定的进展，但仍面临着诸多挑战，如语义理解、语法结构、韵律要求等。未来，随着深度学习技术的不断发展和对联生成应用场景的不断拓展，这些挑战将逐渐得到解决，对联生成系统的性能和应用前景也将不断提升。</p><h2 id="二、相关理论及其技术介绍"><a href="#二、相关理论及其技术介绍" class="headerlink" title="二、相关理论及其技术介绍"></a>二、相关理论及其技术介绍</h2><h3 id="2-1-循环神经网络"><a href="#2-1-循环神经网络" class="headerlink" title="2.1 循环神经网络"></a>2.1 循环神经网络</h3><h4 id="2-1-1-标准循环神经网络"><a href="#2-1-1-标准循环神经网络" class="headerlink" title="2.1.1 标准循环神经网络"></a>2.1.1 标准循环神经网络</h4><p>​基础的神经网络包括输入层、隐藏层和输出层三层结构，其只在层与层之间建立连接。而标准循环神经网络 RNN(Recurrent Neural Network)在此基础上，在同层之间的神经元之间也建立了连接。RNN 的神经网络结构如图 2.1 所示，等号右边为神经网络按时间展开图，等号左边是其简化图。</p><p><img src="/../images/postsImg/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/clip_image002.jpg" alt="img"></p><h4 id="2-1-2-长短时记忆网络"><a href="#2-1-2-长短时记忆网络" class="headerlink" title="2.1.2 长短时记忆网络"></a>2.1.2 长短时记忆网络</h4><p>​长短时记忆网络 LSTM 是循环神经网络的一个最常见的扩展模型，其就是为了解决一般 RNN 网络的缺陷，更有效的应对产期依赖问题。LSTM的隐藏层比RNN更为复杂，采用了三个“门”结构，用来解决长期依赖问题。如图2.2所示LSTM的隐藏层分别包含了输入门、遗忘门和输出门。</p><p><img src="/../images/postsImg/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/clip_image002-17337985776821.jpg" alt="img"></p><p>​这是最基本的 LSTM 结构，目前有 LSTM 的变种形式如 GRU 等等，但是基本思想都是统一的。LSTM 和 GRU 是目前比较常见的神经网络隐藏层单元结构，除此以外，还有 Koutnik提出的 Clockwork RNN 结构，Yao 提出的 Depth Gated RNN 结构等[2]，对于具体的任务，可以适当变形以适应任务需要。</p><h3 id="2-2-注意力机制"><a href="#2-2-注意力机制" class="headerlink" title="2.2 注意力机制"></a>2.2 注意力机制</h3><p>​注意力机制（Attention Mechanism，简称 AM）最早的提出是在图像领域，受到人类注意力机制的启发，人类观察图像时往往有重点的将注意力集中在图像的特定部分并且会根据之前观察图像的经验来观察之后的图像。基于注意力机制的文本分类机制为每个词赋予了权重α，并根据权重生成语义编码C。其编码的三个阶段具体计算步骤为：</p><ol><li>计算 Query和不同 Key 的相关性，计算不同 Value 值的权重系数。</li><li>对上一阶段的输出进行归一化处理，将数值的范围映射到 0 和 1 之间。</li><li>根据权重系数对Value进行加权求和，从而得到最终的注意力数值。</li></ol><p>​在注意力模型中，由于每一次输出的词语在计算的时候，使用到的语义编码C 都是不一样的，这也体现出注意力的意义，在很多任务中都已经取得了很好的效果。</p><h3 id="2-3-序列到序列模型"><a href="#2-3-序列到序列模型" class="headerlink" title="2.3 序列到序列模型"></a>2.3 序列到序列模型</h3><h4 id="2-3-1-经典的序列到序列模型"><a href="#2-3-1-经典的序列到序列模型" class="headerlink" title="2.3.1 经典的序列到序列模型"></a>2.3.1 经典的序列到序列模型</h4><p>​序列到序列模型（seq2seq）亦称为编码-解码模型，由谷歌公司于2014年在论文《Sequence to sequence learning with neural networks》中提出。序列到序列模型的提出在整个深度学习领域获得了很大的影响。近年来在机器翻译、语音识别、图像识别、文本生成等领域的大部分研究都是围绕序列到序列的模型框架展开。如图2.3所展示了一个经典的序列到序列模型。模型包含了一个编码器（Encoder）和一个解码器（Decoder），编码器的作用是提取输入序列特征，并将其压缩成一个固定大小的语义向量C，解码器的作用是解读语义向量C，将其转化为目标输出序列。其中编码器和解码器一般采用循环神经网络[3]。</p><p><img src="/../images/postsImg/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/clip_image001.png" alt="img"></p><h4 id="2-3-2-基于注意力机制的序列到序列模型"><a href="#2-3-2-基于注意力机制的序列到序列模型" class="headerlink" title="2.3.2 基于注意力机制的序列到序列模型"></a>2.3.2 基于注意力机制的序列到序列模型</h4><p>​经典的序列到序列模型在很多问题上都非常有效，但是也存在问题。最大的不足在于，编码器和解码器仅通过一个固定的语义向量C进行联系，每个输出时刻语义向量的值都是不变的，且语义向量并不能够将整个序列的信息完全表示，如此解码器在每个时刻仅根据历史的信息和固定的语义向量C进行输出，这样会导致本应该和输出强相关的输入信息被严重的稀释了[4]。</p><p>​因此将注意力机制应用于序列到序列模型，如图2.4展示了基于注意力机制的序列到序列模型示意图。用经典的序列到序列模型结合注意力机制，使得编码器和解码器之间通过一个可变的语义向量进行联系，每个输出时刻语义向量都是不同的，可变的Ci表示了每个时刻输出最相关的输入信息。</p><p><img src="/../images/postsImg/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/clip_image001-17337987235392.png" alt="img"></p><p>​本文主要用到基于注意力机制的序列到序列模型，因此这里展开描述基于注意力机制的序列到序列模型计算过程步骤：</p><p>​假设基于注意力机制的序列到序列模型输入的源序列为S &#x3D; (x1,x2,…,xm)，输出的目标序列为T&#x3D; (y1,y2,…,yn)，那么在第i时刻模型的输出计算过程可以表示如下：</p><p><img src="/../images/postsImg/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/clip_image001-17337987355173.png" alt="img"></p><p>​其中，Ci不是一个固定的值，会根据每一个时刻的变化而变化，由编码器的隐藏向量(h1,…,hm)通过如下公式计算得到：</p><p><img src="/../images/postsImg/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/clip_image001-17337987461764.png" alt="img"></p><p>​其中，𝛼i𝑗为注意力权值，计算公式如下：</p><p><img src="/../images/postsImg/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/clip_image001-17337987565385.png" alt="img"></p><p>​其中，</p><p><img src="/../images/postsImg/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/clip_image001-17337987685056.png" alt="img"></p><p>​其中，𝑠i−1表示解码器第i-1时刻的隐藏层状态，函数F(𝑎, 𝑏)表示计算a和b的相似度。首先计算解码器隐藏层状态和编码器隐藏层状态的相似度，然后对计算出的相似度采用类Softmax函数进行归一化，得到注意力权值，最后注意力权值与编码器隐藏状态相乘求和得到语义向量Ct。</p><h2 id="三、对联生成模型设计"><a href="#三、对联生成模型设计" class="headerlink" title="三、对联生成模型设计"></a>三、对联生成模型设计</h2><h3 id="3-1-Encoder-Decoder-框架结构简介"><a href="#3-1-Encoder-Decoder-框架结构简介" class="headerlink" title="3.1 Encoder-Decoder 框架结构简介"></a>3.1 Encoder-Decoder 框架结构简介</h3><p>​对联生成问题为典型的序列生成问题，这里采用最经典的 Encoder-Decoder框架结构，由于预测句子中下一个词一般需要用到前面已经生成的词，前后词语之间并不是独立的，RNN 同层节点之间相互连接能够体现这种前后词语之间的联系，并且同层节点之间的权值是共享的，因此本文任务选择使用 RNN Encoder-Decoder。一般的 RNN 结构需要将输入压缩成一个固定长度的向量，这就很难处理一些比较长的句子，特别是那些比训练集中语句更长的语句，本文在编码阶段采用的是双向RNN结构——BiGRU，这在语音识别方面已经得到了很有效的应用，解码阶段选择的就是GRU 结构。为了在每次生成输出词语的时候充分利用输入序列携带的信息，使输入序列中的不同词语对输出词语有不同的影响，本文在解码阶段使用了注意力机制。另外，为了体现上下联语句在语义和语境上的一致性，本文在解码的时候将含有上联整句信息的句向量考虑进去，下图是本文建立的对联生成模型的总体结构图。</p><p> <img src="/../images/postsImg/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/clip_image002-17337988378657.jpg" alt="img"></p><h4 id="3-1-1-输入处理"><a href="#3-1-1-输入处理" class="headerlink" title="3.1.1 输入处理"></a>3.1.1 输入处理</h4><p>​本文任务的基本模式就是，输入上联输出对应的下联，这里的输入上联是汉字组成的序列。很显然，将这个任务转化为深度学习问题，必须将上联的文字符号化输入计算机中。第一步任务就是将序列中汉字转化为一个向量，就是所谓的词向量。一般的词向量在训练时只考虑到词语与其所在训练序列中的前后词语之间的关系，由于对联任务的特殊性，不仅需要考虑前后词语之间的关系，还需要考虑上下联中对应位置的词语之间的联系，于是，在训练词向量的时候将上下联对应位置的字考虑在内，也就是将其加入上下文中，将训练出来的这种词向量称之为对联字向量。</p><h4 id="3-1-2-编码阶段"><a href="#3-1-2-编码阶段" class="headerlink" title="3.1.2 编码阶段"></a>3.1.2 编码阶段</h4><p>​为了体现序列前后词语之间的联系，在编码阶段本文使用的是BiGRU 的结构。门控循环单元神经网络（gated recurrent unit neural network，简称 GRU）是由 Cho 等人在 2014 年提出来的，是 RNN 的衍生物，属于 RNN 的一种变种，与 RNN 的结构本质上并没有什么不同，差异在于计算隐藏层状态的函数。</p><p>​GRU 使用了更新门（update gate）与重置门（reset gate）来控制标准 RNN 的梯度消失的问题，这两个门控向量基本上决定了 GRU 最终输出哪些信息。这两门控机制能够保存长期序列中的信息，且不会随着时间变长而被清除或因与预测不相关而被移除。</p><p>​本文中编码阶段使用的是双向的GRU，由图4.1可以看出，一个前向的GRU，一个反向的 GRU，正向的和反向的 GRU 实际上并没有什么区别，只是输入序列的顺序不同。</p><h4 id="3-1-3-解码阶段"><a href="#3-1-3-解码阶段" class="headerlink" title="3.1.3 解码阶段"></a>3.1.3 解码阶段</h4><p>​解码阶段的任务是根据输入 X 的中间语义表示 C 以及前面生成的历史信息（y1,y2,…,yi-1）来生成 i 时刻要输出的词语 yi，前面说过，解码阶段使用的依然是RNN 的变体 GRU。在基础的 Decoder 中，每个 ci 都一模一样的，并不能加强或者减轻某个字对某个词的作用大小，很明显上联中对应位置的字对当前字生成的影响较大，其他位置的字对当前字生成的影响较小。为了解决这个问题，体现出上联中不同字对于下联中不同词语的影响，在解码过程中应用了 Attention 机制[5]。AM 模型可以说是今年来 NLP 领域中的重要进展之一，其效果在很多场景得到证实。在 Decoder中加入 AM 后，在生成每一个 yi 的时候的中间语义向量 ci 都是不相同的，是根据当前生成字而不断变化的[6]。</p><h3 id="3-2-数据处理部分"><a href="#3-2-数据处理部分" class="headerlink" title="3.2 数据处理部分"></a>3.2 数据处理部分</h3><h4 id="3-2-1-噪声数据摘除"><a href="#3-2-1-噪声数据摘除" class="headerlink" title="3.2.1 噪声数据摘除"></a>3.2.1 噪声数据摘除</h4><p>​数据集来源：<a href="https://github.com/wb14123/couplet-dataset">https://github.com/wb14123/couplet-dataset</a> 此数据集包含了五个文件，将文件按照（test+train）的顺序进行合并[7]。之后发现有14条数据有问题，其中前10个问题为上联，后4个问题为下联。经过对噪声数据进行处理后，最终获得744915条对联数据。</p><p><img src="/../images/postsImg/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/clip_image002.png" alt="img"></p><h4 id="3-2-2-添加输入开始与结束标志"><a href="#3-2-2-添加输入开始与结束标志" class="headerlink" title="3.2.2 添加输入开始与结束标志"></a>3.2.2 添加输入开始与结束标志</h4><p>&lt; start&gt; 表示一个输入的开始 </p><p>&lt; end&gt; 表示一个输入的结束</p><p><img src="/../images/postsImg/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/clip_image002-17337989826658.png" alt="img"></p><h4 id="3-2-3-建立词库与字典"><a href="#3-2-3-建立词库与字典" class="headerlink" title="3.2.3 建立词库与字典"></a>3.2.3 建立词库与字典</h4><p>​在字典的创建过程中，主要先用一个辅助字典，存储每个词汇出现的频率，然后根据频率排序辅助字典。然后初始化两个字典，遍历辅助字典,可用当前字典（非辅助字典）的长度作为词汇数字标识。 在该过程中，我们还可以根据频率筛选出频率大于一定阈值的词汇，即抛弃低频词汇[8]。</p><p>​词库：由上下联所有字符，可用列表表示</p><p>​字典：建立词汇–&gt;向量，向量–&gt;词汇两个字典。向量在这里可以理解为数字化，最终的目的是向量化。</p><p><img src="/../images/postsImg/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/clip_image002-17337990023979.png" alt="img"></p><h4 id="3-2-4-数据向量化"><a href="#3-2-4-数据向量化" class="headerlink" title="3.2.4 数据向量化"></a>3.2.4 数据向量化</h4><p>​根据前一步中创建的“词汇–&gt;向量”字典将所有数据向量化。</p><p><img src="/../images/postsImg/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/clip_image002-173379901873010.jpg" alt="img"></p><h4 id="3-2-5-数据集划分及封装"><a href="#3-2-5-数据集划分及封装" class="headerlink" title="3.2.5 数据集划分及封装"></a>3.2.5 数据集划分及封装</h4><p>​我们总的数据为744915条，按照8:1:1的方式划分训练集，验证集，测试集。数据集的封装主要使用paddlepaddle以及paddlenlp提供的方法,其中paddlenlp主要是进行每个minibath数据的padding操作，统一长度[9]。</p><p>​最终我们得到封装好的训练集、验证集，测试集。以训练集为例，每个元素包括五部分：</p><ol><li>上联输入向量</li><li>输入向量未填充前的长度，用于控制LSTM的隐藏状态是否更新填充位置</li><li>下联输入向量，不包含最后一个位置的元素</li><li>下联输入向量，不包含第一个元素，然后在最后一个扩维，用于loss计算</li><li>下联输入向量的mask向量</li></ol><h2 id="四、模型训练"><a href="#四、模型训练" class="headerlink" title="四、模型训练"></a>四、模型训练</h2><h3 id="4-1-参数设定"><a href="#4-1-参数设定" class="headerlink" title="4.1 参数设定"></a>4.1 参数设定</h3><ul><li>num_layers&#x3D;2:LSTM的层数</li><li>hidden_size&#x3D;128:隐藏层的状态数</li><li>embedding_dim&#x3D;256:嵌入层的维度</li><li>lr&#x3D;0.001:学习率</li><li>log_freq&#x3D;200:每200个batch输入一次日志信息</li><li>max_grad_norm&#x3D;5:梯度裁剪</li><li>optimizer&#x3D;Adam():优化器，不是参数，在这里我们把他看作一个参数</li><li>loss&#x3D;CrossEntropy():损失函数，使用的是带掩码的交叉熵损失函数</li><li>metrics&#x3D;Preplexity():评价准则，困惑度</li></ul><h3 id="4-2-模型训练"><a href="#4-2-模型训练" class="headerlink" title="4.2 模型训练"></a>4.2 模型训练</h3><p>部分关键代码：</p><p><img src="/../images/postsImg/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/clip_image002-173379911835411.jpg" alt="img"></p><p>使用model.fit()函数进行训练及验证。</p><p>部分参数设置：</p><ul><li>epcohs&#x3D;20</li><li>eos_id&#x3D;word2id_dict[‘<end>‘]</end></li><li>num_layers&#x3D;2</li><li>dropout_rate&#x3D;0.2</li><li>hidden_size&#x3D;128</li><li>embedding_dim&#x3D;256</li><li>max_grad_norm&#x3D;5</li><li>lr&#x3D;0.001</li><li>log_freq&#x3D;200</li></ul><p><img src="/../images/postsImg/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/clip_image002-173379914079712.png" alt="img"></p><h3 id="4-3-模型预测"><a href="#4-3-模型预测" class="headerlink" title="4.3 模型预测"></a>4.3 模型预测</h3><p>​模型的预测使用了beam search(束搜索)。常规的搜索方法有greedy search（贪心搜索）和exhaustive search（穷举搜索）。</p><p>​穷举搜索：穷举所有可能的输出结果。例如输出序列长度为3，候选项为4，那么就有4<em>4</em>4&#x3D;64种可能，当输出序列长度为10时，就会有4**10种可能，这种幂级增长对于计算机性能的要求是极高的，耗时耗力。</p><p>​贪心搜索：每次选择概率最大的候选者作为输出。搜索空间小，以局部最优解期望全局最优解，无法保证最终结果是做优的，但是效率高。</p><p>​束搜索：束搜索可以看作是穷举搜索和贪心搜索的折中方案。需要设定一个beam size(束宽)，当设为1时即为贪心搜索，当设为候选项的数量时即为穷举搜索。</p><h3 id="4-4-展示界面搭建"><a href="#4-4-展示界面搭建" class="headerlink" title="4.4 展示界面搭建"></a>4.4 展示界面搭建</h3><p>​展示界面使用Python自带的TKinter包进行搭建，主要包括两个文本框和两个动作按钮。用户输入对联的上联，然后点击相应按钮，系统会提取用户的输入，将其向量化，然后送入训练好的模型中，产生输出，然后显示在另一个文本框中。</p><p><img src="/../images/postsImg/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/clip_image002-173379917879313.jpg" alt="img"></p><h2 id="五、模型评测"><a href="#五、模型评测" class="headerlink" title="五、模型评测"></a>五、模型评测</h2><h3 id="5-1-loss曲线"><a href="#5-1-loss曲线" class="headerlink" title="5.1 loss曲线"></a>5.1 loss曲线</h3><p>​loss曲线数据由PaddlePaddle提供的接口在训练时保存在log文件中，之后通过可视化工具进行展示。横轴表示训练的minibatch，纵轴表示loss值。</p><p>​通过训练集与验证集的lossqu曲线可以看出，训练在前期收敛较快，训练集后期有波动，但是验证集后期仍为缓慢下降趋势，说明模型的训练效果是不错的。</p><p><img src="/../images/postsImg/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/clip_image002-173379922392814.png" alt="img"></p><p><img src="/../images/postsImg/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/clip_image002-173379922806515.png" alt="img"></p><h3 id="5-2-困惑度"><a href="#5-2-困惑度" class="headerlink" title="5.2 困惑度"></a>5.2 困惑度</h3><p>​困惑度数据的获取方式同loss。通过训练集与验证集的困惑度曲线，任务模型训练效果可以。但是训练集的困惑度后期呈现为直线，这应该是存在问题的，有待分析解决。我们在测试集上进行了困惑度分析，每个batch的数据困惑度基本一致，说明模型波动较小，效果理想。</p><p><img src="/../images/postsImg/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/clip_image002-173379924722916.png" alt="img"></p><p><img src="/../images/postsImg/%E5%9F%BA%E4%BA%8E%E5%BE%AA%E7%8E%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E5%AF%B9%E8%81%94%E8%87%AA%E5%8A%A8%E7%94%9F%E6%88%90%E7%A0%94%E7%A9%B6/clip_image002-173379925096617.png" alt="img"></p><h2 id="六、结语"><a href="#六、结语" class="headerlink" title="六、结语"></a>六、结语</h2><p>​对联是中华文化一种独特的艺术形式，其上下联之间讲究对仗工整、平仄协调，这要求对联创作者具备丰富的知识储备和深厚的文学素养，因此创作对联对普通人来说稍显困难。对于计算机来说，在自然语言处理领域，对联的生成也是一项比较困难的任务。近年来，深度学习技术快速发展，在如图像识别、语音识别等机器学习领域表现出色，自然语言处理作为机器学习的重要分支，深度学习技术也推动着自然语言处理技术不断发展。</p><p>​本文首先讨论了自动生成对联的研究背景和研究意义，以及对联生成模型的国内外研究现状。介绍了循环神经网络和注意力机制，并且采用基于注意力机制的序列到序列模型对对联生成系统进行进一步研究。掌握了基于编码-解码框架的神经网络模型、注意力机制模型等算法，明确研究方向，舍弃了传统的基于循环神经网络或卷积神经网络的方法，完全使用注意力机制的神经网络结构进行对联的自动生成。结果表明注意力机制在对联的自动生成任务上具有不可替代的作用。</p><h2 id="七、参考文献"><a href="#七、参考文献" class="headerlink" title="七、参考文献"></a>七、参考文献</h2><p>[1]   Manurung, R., Ritchie, G. and Thompson, H. (2012) Using genetic algorithms to create meaningful poetic text. J. Exp. Theor. Artif. Intel., 24, 43–64.</p><p>[2]   Sundermeyer, M., Schlüter, R. and Ney, H. (2012) LSTM Neural Networks for Language Modeling. In Proceedings of Interspeech 2012, Portland, OR, 9–13 September, pp. 601–608. Association for Computational Linguistics, Stroudsburg.</p><p>[3]   Papineni, K., Roukos, S., Ward, T. and Zhu, W.J. (2002) BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, Philadelphia, Pennsylvania, USA, 19–24 July 2002, pp. 311–318. Association for Computational Linguistics, Stroudsburg.</p><p>[4]   王治权.基于注意力机制和改进型 RNN 的 Web 文本情感分析研究[D]. 2018.</p><p>[5]   王哲.基于深度学习技术的中国传统诗歌生成方法研究[D].中国科学技术大学,2017.</p><p>[6]   Koutnik J, Greff K, Gomez F, et al. A clockwork rnn[J]. arXiv preprint arXiv:1402.3511, 2014.</p><p>[7]   蒋锐滢,崔磊,何晶,等.基于主题模型和统计机器翻译方法的中文格律诗自动生成 [J].计算机学报, 2015.</p><p>[8]   Oliveira, H.G., Hervas, R., Diaz, A. and Gervas, P. (2017)Multilingual extension and evaluation of a poetry generator. Nat.Lang. Eng., 23, 929–967.</p><p>[9]   Zhang J, Du J, Dai L. A GRU-based Encoder-Decoder Approach with Attention for Online Handwritten Mathematical Expression Recognition[J]. 2017.</p>]]></content>
      
      
      <categories>
          
          <category> 学习日常 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 作业 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 循环神经网络 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>决策树实验</title>
      <link href="/2024/05/23/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/"/>
      <url>/2024/05/23/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<h1 id="实验四-决策树实验"><a href="#实验四-决策树实验" class="headerlink" title="实验四 决策树实验"></a><strong>实验四 决策树实验</strong></h1><h2 id="一、实验目的"><a href="#一、实验目的" class="headerlink" title="一、实验目的"></a>一、实验目的</h2><p>1.认识决策树的构建过程；</p><p>2.分析决策树可视化的运算；</p><p>3.掌握决策树的应用。</p><h2 id="二、实验内容"><a href="#二、实验内容" class="headerlink" title="二、实验内容"></a>二、实验内容</h2><p>​决策树是什么？决策树(decision tree)是一种基本的分类与回归方法。举个通俗易懂的例子，如下图所示的流程图就是一个决策树，长方形代表判断模块，椭圆形成代表终止模块，表示已经得出结论，可以终止运行。从判断模块引出的左右箭头称作为分支，它可以达到另一个判断模块或者终止模块。我们还可以这样理解，分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部结点和叶结点。内部结点表示一个特征或属性，叶结点表示一个类。</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209145103826.png" alt="image-20241209145103826"></p><p>​使用决策树做预测需要以下过程：</p><ol><li><p>收集数据：可以使用任何方法。比如想构建一个系统，我们可以从亲朋好友那里获取数据。根据他们考虑的因素和最终的选择结果，就可以得到一些供我们利用的数据了。</p></li><li><p>准备数据：收集完的数据，我们要进行整理，将这些所有收集的信息按照一定规则整理出来，并排版，方便我们进行后续处理。</p></li><li><p>分析数据：可以使用任何方法，决策树构造完成之后，我们可以检查决策树图形是否符合预期。</p></li><li><p>训练算法：这个过程也就是构造决策树，同样也可以说是决策树学习，就是构造一个决策树的数据结构。</p></li><li><p>测试算法：使用经验树计算错误率。当错误率达到了可接收范围，这个决策树就可以投放使用了。</p></li><li><p>使用算法：此步骤可以使用适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。</p></li></ol><h2 id="三、实验材料与工具"><a href="#三、实验材料与工具" class="headerlink" title="三、实验材料与工具"></a>三、实验材料与工具</h2><p>1．电脑一台；</p><p>2．Pycharm。</p><h2 id="四、实验步骤"><a href="#四、实验步骤" class="headerlink" title="四、实验步骤"></a>四、实验步骤</h2><p><strong>1、分析以下数据</strong></p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209145712953.png" alt="image-20241209145712953"></p><p>​特征选择</p><p>​特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率，如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征没有分类能力。扔掉这样的特征对决策树学习的精度影响不大。通常特征选择的标准是信息增益(information gain)或信息增益比，本次实验使用信息增益作为选择特征的标准。那么，什么是信息增益？在讲解信息增益之前，让我们看一组实例，贷款申请样本数据表。</p><p>​希望通过所给的训练数据学习一个贷款申请的决策树，用于对未来的贷款申请进行分类，即当新的客户提出贷款申请时，根据申请人的特征利用决策树决定是否批准贷款申请。</p><p>​特征选择就是决定用哪个特征来划分特征空间。比如，我们通过上述数据表得到两个可能的决策树，分别由两个不同特征的根结点构成。</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209145748807.png" alt="image-20241209145748807"></p><p>​左图所示的根结点的特征是年龄，有3个取值，对应于不同的取值有不同的子结点。右图所示的根节点的特征是工作，有2个取值，对应于不同的取值有不同的子结点。两个决策树都可以从此延续下去。问题是：究竟选择哪个特征更好些？这就要求确定选择特征的准则。直观上，如果一个特征具有更好的分类能力，或者说，按照这一特征将训练数据集分割成子集，使得各个子集在当前条件下有最好的分类，那么就更应该选择这个特征。信息增益就能够很好地表示这一直观的准则。</p><p>​什么是信息增益呢？在划分数据集之后信息发生的变化称为信息增益，知道如何计算信息增益，我们就可以计算每个特征值划分数据集获得的信息增益，获得信息增益最高的特征就是最好的选择。</p><p><strong>2、计算香农熵</strong></p><p>​在可以评测哪个数据划分方式是最好的数据划分之前，我们必须学习如何计算信息增益。集合信息的度量方式称为香农熵或者简称为熵(entropy)，这个名字来源于信息论之父克劳德·香农。</p><p>​熵定义为信息的期望值。在信息论与概率统计中，熵是表示随机变量不确定性的度量。如果待分类的事物可能划分在多个分类之中，则符号xi的信息定义为 ：</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209145827471.png" alt="image-20241209145827471"></p><p>​其中p(xi)是选择该分类的概率。上述式中的对数以2为底，也可以e为底(自然对数)。</p><p>​通过上式，我们可以得到所有类别的信息。为了计算熵，我们需要计算所有类别所有可能值包含的信息期望值(数学期望)，通过下面的公式得到：</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209145840117.png" alt="image-20241209145840117"></p><p>​其中n是分类的数目。熵越大，随机变量的不确定性就越大。</p><p>​当熵中的概率由数据估计(特别是最大似然估计)得到时，所对应的熵称为经验熵(empirical entropy)。比如有10个数据，一共有两个类别，A类和B类。其中有7个数据属于A类，则该A类的概率即为十分之七。其中有3个数据属于B类，则该B类的概率即为十分之三。这概率是我们根据数据数出来的。我们定义贷款申请样本数据表中的数据为训练数据集D，则训练数据集D的经验熵为H(D)，|D|表示其样本容量，及样本个数。设有K个类Ck, &#x3D; 1,2,3,…,K,|Ck|为属于类Ck的样本个数，因此经验熵公式就可以写为 ：</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209145900014.png" alt="image-20241209145900014"></p><p>​根据此公式计算经验熵H(D)，分析贷款申请样本数据表中的数据。最终分类结果只有两类，即放贷和不放贷。根据表中的数据统计可知，在15个数据中，9个数据的结果为放贷，6个数据的结果为不放贷。所以数据集D的经验熵H(D)为：</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209145916351.png" alt="image-20241209145916351"></p><p>​经过计算可知，数据集D的经验熵H(D)的值为0.971。</p><p><strong>3、编写代码计算经验熵</strong></p><p>​首先对数据集进行属性标注。</p><ul><li><p>年龄：0代表青年，1代表中年，2代表老年；</p></li><li><p>有工作：0代表否，1代表是；</p></li><li><p>有自己的房子：0代表否，1代表是；</p></li><li><p>信贷情况：0代表一般，1代表好，2代表非常好；</p></li><li><p>类别(是否给贷款)：no代表否，yes代表是。</p><p>​然后创建数据集，并计算经验熵。</p><p>​代码编写如下：</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib.font_manager <span class="keyword">import</span> FontProperties  </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  </span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log  </span><br><span class="line"><span class="keyword">import</span> operator  </span><br><span class="line"><span class="keyword">import</span> pickle  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createDataSet</span>():  </span><br><span class="line">    dataSet = [[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="string">&#x27;no&#x27;</span>],  </span><br><span class="line">               [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="string">&#x27;no&#x27;</span>],  </span><br><span class="line">               [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="string">&#x27;no&#x27;</span>],  </span><br><span class="line">               [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="string">&#x27;no&#x27;</span>],  </span><br><span class="line">               [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="string">&#x27;no&#x27;</span>],  </span><br><span class="line">               [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">2</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="string">&#x27;no&#x27;</span>]  </span><br><span class="line">              ]  </span><br><span class="line">    labels = [<span class="string">&#x27;年龄&#x27;</span>,<span class="string">&#x27;有工作&#x27;</span>,<span class="string">&#x27;有自己的房子&#x27;</span>,<span class="string">&#x27;信贷情况&#x27;</span>]  </span><br><span class="line">    <span class="keyword">return</span> dataSet,labels  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcShannonEnt</span>(<span class="params">dataSet</span>):  </span><br><span class="line">    numEntires = <span class="built_in">len</span>(dataSet)  </span><br><span class="line">    labelCounts = &#123;&#125;  </span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:  </span><br><span class="line">        currentLabel = featVec[-<span class="number">1</span>]  </span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():  </span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span>  </span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span>  </span><br><span class="line">    shannonEnt = <span class="number">0.0</span>  </span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:  </span><br><span class="line">        prob = <span class="built_in">float</span>(labelCounts[key]) / numEntires  </span><br><span class="line">        shannonEnt -= prob * log(prob,<span class="number">2</span>)  </span><br><span class="line">    <span class="keyword">return</span> shannonEnt </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    dataSet,labels = createDataSet()</span><br><span class="line">    <span class="built_in">print</span>(dataSet)</span><br><span class="line">    <span class="built_in">print</span>(calcShannonEnt(dataSet))</span><br></pre></td></tr></table></figure><p>​代码运行结果如下图所示，代码是先打印训练数据集，然后打印计算的经验熵H(D)，程序计算的结果与我们统计计算的结果是一致的，程序没有问题。</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209150622268.png" alt="image-20241209150622268"></p><p><strong>4、计算信息增益</strong></p><p>​在上面，我们已经说过，如何选择特征，需要看信息增益。也就是说，信息增益是相对于特征而言的，信息增益越大，特征对最终的分类结果影响也就越大，我们就应该选择对最终分类结果影响最大的那个特征作为我们的分类特征。</p><p>​条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性，随机变量X给定的条件下随机变量Y的条件熵(conditional entropy)H(Y|X)，定义为X给定条件下Y的条件概率分布的熵对X的数学期望：</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209150645183.png" alt="image-20241209150645183"></p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209150648200.png" alt="image-20241209150648200"></p><p>​当条件熵中的概率由数据估计(特别是极大似然估计)得到时，所对应的条件熵称为条件经验熵(empirical conditional entropy)。</p><p>​明确了条件熵和经验条件熵的概念。接下来，说说信息增益。前面也提到了，信息增益是相对于特征而言的。所以，特征A对训练数据集D的信息增益g(D,A)，定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差，即：</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209150704905.png" alt="image-20241209150704905"></p><p>​熵H(D)与条件熵H(D|A)之差称为互信息(mutual information)。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。</p><p>​设特征A有n个不同的取值{a1,a2,···,an}，根据特征A的取值将D划分为n个子集{D1,D2，···,Dn}，|Di|为Di的样本个数。记子集Di中属于Ck的样本的集合为Dik，即Dik &#x3D; Di ∩ Ck，|Dik|为Dik的样本个数。于是经验条件熵的公式可以写为：</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209150723981.png" alt="image-20241209150723981"></p><p>​以贷款申请样本数据表为例进行说明。看下年龄这一列的数据，也就是特征A1，一共有三个类别，分别是：青年、中年和老年。我们只看年龄是青年的数据，年龄是青年的数据一共有5个，所以年龄是青年的数据在训练数据集出现的概率是十五分之五，也就是三分之一。同理，年龄是中年和老年的数据在训练数据集出现的概率也都是三分之一。现在我们只看年龄是青年的数据的最终得到贷款的概率为五分之二，因为在五个数据中，只有两个数据显示拿到了最终的贷款，同理，年龄是中年和老年的数据最终得到贷款的概率分别为五分之三、五分之四。所以计算年龄的信息增益，过程如下：</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209150737040.png" alt="image-20241209150737040"></p><p>​同理，计算其余特征的信息增益g(D,A2)、g(D,A3)和g(D,A4)。分别为：</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209150748169.png" alt="image-20241209150748169"></p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209150752446.png" alt="image-20241209150752446"></p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209150755655.png" alt="image-20241209150755655"></p><p>​最后，比较特征的信息增益，由于特征A3(有自己的房子)的信息增益值最大，所以选择A3作为最优特征。</p><p><strong>5、编写代码计算信息增益</strong></p><p>​我们已经学会了通过公式计算信息增益，接下来编写代码，计算信息增益。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calcShannonEnt</span>(<span class="params">dataSet</span>):  </span><br><span class="line">    numEntires = <span class="built_in">len</span>(dataSet)  </span><br><span class="line">    labelCounts = &#123;&#125;  </span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:  </span><br><span class="line">        currentLabel = featVec[-<span class="number">1</span>]  </span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():  </span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span>  </span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span>  </span><br><span class="line">    shannonEnt = <span class="number">0.0</span>  </span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:  </span><br><span class="line">        prob = <span class="built_in">float</span>(labelCounts[key]) / numEntires  </span><br><span class="line">        shannonEnt -= prob * log(prob,<span class="number">2</span>)  </span><br><span class="line">    <span class="keyword">return</span> shannonEnt  </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createDataSet</span>():  </span><br><span class="line">    dataSet = [[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="string">&#x27;no&#x27;</span>],  </span><br><span class="line">               [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="string">&#x27;no&#x27;</span>],  </span><br><span class="line">               [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="string">&#x27;no&#x27;</span>],  </span><br><span class="line">               [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="string">&#x27;no&#x27;</span>],  </span><br><span class="line">               [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="string">&#x27;no&#x27;</span>],  </span><br><span class="line">               [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">2</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="string">&#x27;no&#x27;</span>]  </span><br><span class="line">              ]  </span><br><span class="line">    labels = [<span class="string">&#x27;年龄&#x27;</span>,<span class="string">&#x27;有工作&#x27;</span>,<span class="string">&#x27;有自己的房子&#x27;</span>,<span class="string">&#x27;信贷情况&#x27;</span>]  </span><br><span class="line">    <span class="keyword">return</span> dataSet,labels  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">splitDataSet</span>(<span class="params">dataSet,axis,value</span>):  </span><br><span class="line">    retDataSet = []  </span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:  </span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == value:  </span><br><span class="line">            reducedFeatVec = featVec[:axis]  </span><br><span class="line">            reducedFeatVec.extend(featVec[axis+<span class="number">1</span>:])  </span><br><span class="line">            retDataSet.append(reducedFeatVec)  </span><br><span class="line">    <span class="keyword">return</span> retDataSet  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chooseBestFeatureToSplit</span>(<span class="params">dataSet</span>):  </span><br><span class="line">    numFeatures = <span class="built_in">len</span>(dataSet[<span class="number">0</span>]) - <span class="number">1</span>  </span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)  </span><br><span class="line">    bestInfoGain = <span class="number">0.0</span>  </span><br><span class="line">    beatFeature = -<span class="number">1</span>  </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numFeatures):  </span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]  </span><br><span class="line">        uniqueVals = <span class="built_in">set</span>(featList)  </span><br><span class="line">        newEntropy = <span class="number">0.0</span>  </span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:  </span><br><span class="line">            subDataSet = splitDataSet(dataSet,i,value)  </span><br><span class="line">            prob = <span class="built_in">len</span>(subDataSet) / <span class="built_in">float</span>(<span class="built_in">len</span>(dataSet))  </span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)  </span><br><span class="line">        infoGain = baseEntropy - newEntropy  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;第%d个特征的增益为%.3f&quot;</span> % (i,infoGain))  </span><br><span class="line">        <span class="keyword">if</span>(infoGain &gt; bestInfoGain):  </span><br><span class="line">            bestInfoGain = infoGain  </span><br><span class="line">            bestFeature = i  </span><br><span class="line">    <span class="keyword">return</span> bestFeature  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:  </span><br><span class="line">    dataSet,labels = createDataSet()  </span><br><span class="line">    <span class="built_in">print</span>(dataSet)  </span><br><span class="line">    <span class="built_in">print</span>(labels)  </span><br><span class="line">    <span class="built_in">print</span>(calcShannonEnt(dataSet))  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;最优特征索引值：&quot;</span> + <span class="built_in">str</span>(chooseBestFeatureToSplit(dataSet))) </span><br></pre></td></tr></table></figure><p>​splitDataSet函数是用来选择各个特征的子集的，比如选择年龄(第0个特征)的青年(用0代表)的自己，我们可以调用splitDataSet(dataSet,0,0)这样返回的子集就是年龄为青年的5个数据集。chooseBestFeatureToSplit是选择选择最优特征的函数。运行代码结果如下：</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209151321875.png" alt="image-20241209151321875"></p><p>​对比我们自己计算的结果，最优特征的索引值为2，也就是特征A3(有自己的房子)。</p><p>​这样就生成了一个决策树，该决策树只用了两个特征(有两个内部结点)，生成的决策树如下图所示。</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209151335494.png" alt="image-20241209151335494"></p><p>​我们已经学习了从数据集构造决策树算法所需要的子功能模块，包括经验熵的计算和最优特征的选择，其工作原理如下：得到原始数据集，然后基于最好的属性值划分数据集，由于特征值可能多于两个，因此可能存在大于两个分支的数据集划分。第一次划分之后，数据集被向下传递到树的分支的下一个结点。在这个结点上，我们可以再次划分数据。因此我们可以采用递归的原则处理数据集。</p><p>​决策树生成算法递归地产生决策树，直到不能继续下去未为止。这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。</p><p><strong>6、构建决策树</strong></p><p>​利用求得的结果，由于特征A3(有自己的房子)的信息增益值最大，所以选择特征A3作为根结点的特征。它将训练集D划分为两个子集D1(A3取值为”是”)和D2(A3取值为”否”)。由于D1只有同一类的样本点，所以它成为一个叶结点，结点的类标记为“是”。</p><p>​对D2则需要从特征A1(年龄)，A2(有工作)和A4(信贷情况)中选择新的特征，计算各个特征的信息增益：</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209151412616.png" alt="image-20241209151412616"></p><p>​根据计算，选择信息增益最大的特征A2(有工作)作为结点的特征。由于A2有两个可能取值，从这一结点引出两个子结点：一个对应”是”(有工作)的子结点，包含3个样本，它们属于同一类，所以这是一个叶结点，类标记为”是”；另一个是对应”否”(无工作)的子结点，包含6个样本，它们也属于同一类，所以这也是一个叶结点，类标记为”否”。</p><p>​这样就生成了一个决策树，该决策树只用了两个特征(有两个内部结点)，生成的决策树如下图所示。</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209151431267.png" alt="image-20241209151431267"></p><p>​这样我们就使用ID3算法构建出来了决策树，我们使用字典存储决策树的结构：</p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209151442098.png" alt="image-20241209151442098"></p><p>​创建函数majorityCnt统计classList中出现此处最多的元素(类标签)，创建函数createTree用来递归构建决策树。编写代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">majorityCnt</span>(<span class="params">classList</span>):  </span><br><span class="line">    classCount = &#123;&#125;  </span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:  </span><br><span class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.keys():classCount[vote] = <span class="number">0</span>  </span><br><span class="line">        classCount[vote] += <span class="number">1</span>  </span><br><span class="line">    sortedClassCount = <span class="built_in">sorted</span>(classCount.items(),key = operator.itemgetter(<span class="number">1</span>),reverse = <span class="literal">True</span>)  </span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createTree</span>(<span class="params">dataSet,labels,featLabels</span>):  </span><br><span class="line">    classList = [example[-<span class="number">1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]  </span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == <span class="built_in">len</span>(classList):  </span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]  </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(dataSet[<span class="number">0</span>]) == <span class="number">1</span> <span class="keyword">or</span> <span class="built_in">len</span>(labels) == <span class="number">0</span>:  </span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)  </span><br><span class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)  </span><br><span class="line">    bestFeatLabel = labels[bestFeat]  </span><br><span class="line">    featLabels.append(bestFeatLabel)  </span><br><span class="line">    myTree = &#123;bestFeatLabel:&#123;&#125;&#125;  </span><br><span class="line">    <span class="keyword">del</span>(labels[bestFeat])  </span><br><span class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]  </span><br><span class="line">    uniqueVals = <span class="built_in">set</span>(featValues)  </span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:  </span><br><span class="line">        subLabels = labels[:]  </span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet,bestFeat,value),subLabels,featLabels)  </span><br><span class="line">    <span class="keyword">return</span> myTree  </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:  </span><br><span class="line">    dataSet,labels = createDataSet()  </span><br><span class="line">    <span class="built_in">print</span>(dataSet)  </span><br><span class="line">    <span class="built_in">print</span>(labels)  </span><br><span class="line">    <span class="built_in">print</span>(calcShannonEnt(dataSet))  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;最优特征索引值：&quot;</span> + <span class="built_in">str</span>(chooseBestFeatureToSplit(dataSet))) </span><br><span class="line">    featLabels = []  </span><br><span class="line">    myTree = createTree(dataSet,labels,featLabels)  </span><br><span class="line">    <span class="built_in">print</span>(myTree)  </span><br></pre></td></tr></table></figure><p>​递归创建决策树时，递归有两个终止条件：第一个停止条件是所有的类标签完全相同，则直接返回该类标签；第二个停止条件是使用完了所有特征，仍然不能将数据划分仅包含唯一类别的分组，即决策树构建失败，特征不够用。此时说明数据纬度不够，由于第二个停止条件无法简单地返回唯一的类标签，这里挑选出现数量最多的类别作为返回值。</p><h2 id="五、实验代码汇总"><a href="#五、实验代码汇总" class="headerlink" title="五、实验代码汇总"></a>五、实验代码汇总</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib.font_manager <span class="keyword">import</span> FontProperties  </span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  </span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log  </span><br><span class="line"><span class="keyword">import</span> operator  </span><br><span class="line"><span class="keyword">import</span> pickle  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createDataSet</span>():  </span><br><span class="line">    dataSet = [[<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="string">&#x27;no&#x27;</span>],  </span><br><span class="line">               [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="string">&#x27;no&#x27;</span>],  </span><br><span class="line">               [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="string">&#x27;no&#x27;</span>],  </span><br><span class="line">               [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="string">&#x27;no&#x27;</span>],  </span><br><span class="line">               [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="string">&#x27;no&#x27;</span>],  </span><br><span class="line">               [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">2</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">2</span>,<span class="string">&#x27;yes&#x27;</span>],  </span><br><span class="line">               [<span class="number">2</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="string">&#x27;no&#x27;</span>]  </span><br><span class="line">              ]  </span><br><span class="line">    labels = [<span class="string">&#x27;年龄&#x27;</span>,<span class="string">&#x27;有工作&#x27;</span>,<span class="string">&#x27;有自己的房子&#x27;</span>,<span class="string">&#x27;信贷情况&#x27;</span>]  </span><br><span class="line">    <span class="keyword">return</span> dataSet,labels  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcShannonEnt</span>(<span class="params">dataSet</span>):  </span><br><span class="line">    numEntires = <span class="built_in">len</span>(dataSet)  </span><br><span class="line">    labelCounts = &#123;&#125;  </span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:  </span><br><span class="line">        currentLabel = featVec[-<span class="number">1</span>]  </span><br><span class="line">        <span class="keyword">if</span> currentLabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():  </span><br><span class="line">            labelCounts[currentLabel] = <span class="number">0</span>  </span><br><span class="line">        labelCounts[currentLabel] += <span class="number">1</span>  </span><br><span class="line">    shannonEnt = <span class="number">0.0</span>  </span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:  </span><br><span class="line">        prob = <span class="built_in">float</span>(labelCounts[key]) / numEntires  </span><br><span class="line">        shannonEnt -= prob * log(prob,<span class="number">2</span>)  </span><br><span class="line">    <span class="keyword">return</span> shannonEnt  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">splitDataSet</span>(<span class="params">dataSet,axis,value</span>):  </span><br><span class="line">    retDataSet = []  </span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:  </span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == value:  </span><br><span class="line">            reducedFeatVec = featVec[:axis]  </span><br><span class="line">            reducedFeatVec.extend(featVec[axis+<span class="number">1</span>:])  </span><br><span class="line">            retDataSet.append(reducedFeatVec)  </span><br><span class="line">    <span class="keyword">return</span> retDataSet  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">chooseBestFeatureToSplit</span>(<span class="params">dataSet</span>):  </span><br><span class="line">    numFeatures = <span class="built_in">len</span>(dataSet[<span class="number">0</span>]) - <span class="number">1</span>  </span><br><span class="line">    baseEntropy = calcShannonEnt(dataSet)  </span><br><span class="line">    bestInfoGain = <span class="number">0.0</span>  </span><br><span class="line">    beatFeature = -<span class="number">1</span>  </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numFeatures):  </span><br><span class="line">        featList = [example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]  </span><br><span class="line">        uniqueVals = <span class="built_in">set</span>(featList)  </span><br><span class="line">        newEntropy = <span class="number">0.0</span>  </span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:  </span><br><span class="line">            subDataSet = splitDataSet(dataSet,i,value)  </span><br><span class="line">            prob = <span class="built_in">len</span>(subDataSet) / <span class="built_in">float</span>(<span class="built_in">len</span>(dataSet))  </span><br><span class="line">            newEntropy += prob * calcShannonEnt(subDataSet)  </span><br><span class="line">        infoGain = baseEntropy - newEntropy  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;第%d个特征的增益为%.3f&quot;</span> % (i,infoGain))  </span><br><span class="line">        <span class="keyword">if</span>(infoGain &gt; bestInfoGain):  </span><br><span class="line">            bestInfoGain = infoGain  </span><br><span class="line">            bestFeature = i  </span><br><span class="line">    <span class="keyword">return</span> bestFeature  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">majorityCnt</span>(<span class="params">classList</span>):  </span><br><span class="line">    classCount = &#123;&#125;  </span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> classList:  </span><br><span class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> classCount.keys():classCount[vote] = <span class="number">0</span>  </span><br><span class="line">        classCount[vote] += <span class="number">1</span>  </span><br><span class="line">    sortedClassCount = <span class="built_in">sorted</span>(classCount.items(),key = operator.itemgetter(<span class="number">1</span>),reverse = <span class="literal">True</span>)  </span><br><span class="line">    <span class="keyword">return</span> sortedClassCount[<span class="number">0</span>][<span class="number">0</span>]  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createTree</span>(<span class="params">dataSet,labels,featLabels</span>):  </span><br><span class="line">    classList = [example[-<span class="number">1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]  </span><br><span class="line">    <span class="keyword">if</span> classList.count(classList[<span class="number">0</span>]) == <span class="built_in">len</span>(classList):  </span><br><span class="line">        <span class="keyword">return</span> classList[<span class="number">0</span>]  </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(dataSet[<span class="number">0</span>]) == <span class="number">1</span> <span class="keyword">or</span> <span class="built_in">len</span>(labels) == <span class="number">0</span>:  </span><br><span class="line">        <span class="keyword">return</span> majorityCnt(classList)  </span><br><span class="line">    bestFeat = chooseBestFeatureToSplit(dataSet)  </span><br><span class="line">    bestFeatLabel = labels[bestFeat]  </span><br><span class="line">    featLabels.append(bestFeatLabel)  </span><br><span class="line">    myTree = &#123;bestFeatLabel:&#123;&#125;&#125;  </span><br><span class="line">    <span class="keyword">del</span>(labels[bestFeat])  </span><br><span class="line">    featValues = [example[bestFeat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]  </span><br><span class="line">    uniqueVals = <span class="built_in">set</span>(featValues)  </span><br><span class="line">    <span class="keyword">for</span> value <span class="keyword">in</span> uniqueVals:  </span><br><span class="line">        subLabels = labels[:]  </span><br><span class="line">        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet,bestFeat,value),subLabels,featLabels)  </span><br><span class="line">    <span class="keyword">return</span> myTree  </span><br><span class="line">  </span><br><span class="line"><span class="comment">#决策树可视化  </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getNumLeafs</span>(<span class="params">myTree</span>):  </span><br><span class="line">    numLeafs = <span class="number">0</span>  </span><br><span class="line">    firstStr = <span class="built_in">next</span>(<span class="built_in">iter</span>(myTree))  </span><br><span class="line">    secondDict = myTree[firstStr]  </span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():  </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(secondDict[key]).__name__==<span class="string">&#x27;dict&#x27;</span>:  </span><br><span class="line">            numLeafs += getNumLeafs(secondDict[key])  </span><br><span class="line">        <span class="keyword">else</span>:  </span><br><span class="line">            numLeafs += <span class="number">1</span>  </span><br><span class="line">    <span class="keyword">return</span> numLeafs  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">getTreeDepth</span>(<span class="params">myTree</span>):  </span><br><span class="line">    maxDepth = <span class="number">0</span>  </span><br><span class="line">    firstStr = <span class="built_in">next</span>(<span class="built_in">iter</span>(myTree))  </span><br><span class="line">    secondDict = myTree[firstStr]  </span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():  </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(secondDict[key]).__name__==<span class="string">&#x27;dict&#x27;</span>:  </span><br><span class="line">            thisDepth = <span class="number">1</span> + getTreeDepth(secondDict[key])  </span><br><span class="line">        <span class="keyword">else</span>:  </span><br><span class="line">            thisDepth = <span class="number">1</span>  </span><br><span class="line">            <span class="keyword">if</span> thisDepth &gt; maxDepth:maxDepth = thisDepth  </span><br><span class="line">    <span class="keyword">return</span> maxDepth  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotNode</span>(<span class="params">nodeTxt,centerPt,parentPt,nodeType</span>):  </span><br><span class="line">    arrow_args = <span class="built_in">dict</span>(arrowstyle=<span class="string">&quot;&lt;-&quot;</span>)  </span><br><span class="line">    font = FontProperties(fname=<span class="string">r&quot;c:\windows\fonts\simkai.ttf&quot;</span>,size=<span class="number">14</span>)  </span><br><span class="line">    createPlot.ax1.annotate(nodeTxt,xy=parentPt,xycoords=<span class="string">&#x27;axes fraction&#x27;</span>,  </span><br><span class="line">                           xytext=centerPt,textcoords=<span class="string">&#x27;axes fraction&#x27;</span>,  </span><br><span class="line">                           va=<span class="string">&quot;center&quot;</span>,ha=<span class="string">&quot;center&quot;</span>,bbox=nodeType,arrowprops=arrow_args,fontproperties=font)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotMidText</span>(<span class="params">cntrPt,parentPt,txtString</span>):  </span><br><span class="line">    xMid = (parentPt[<span class="number">0</span>]-cntrPt[<span class="number">0</span>])/<span class="number">2.0</span> + cntrPt[<span class="number">0</span>]  </span><br><span class="line">    yMid = (parentPt[<span class="number">1</span>]-cntrPt[<span class="number">1</span>])/<span class="number">2.0</span> + cntrPt[<span class="number">1</span>]  </span><br><span class="line">    createPlot.ax1.text(xMid,yMid,txtString,va=<span class="string">&quot;center&quot;</span>,ha=<span class="string">&quot;center&quot;</span>,rotation=<span class="number">30</span>)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plotTree</span>(<span class="params">myTree,parentPt,nodeTxt</span>):  </span><br><span class="line">    decisionNode = <span class="built_in">dict</span>(boxstyle=<span class="string">&quot;sawtooth&quot;</span>,fc=<span class="string">&quot;0.8&quot;</span>)  </span><br><span class="line">    leafNode = <span class="built_in">dict</span>(boxstyle=<span class="string">&quot;round4&quot;</span>,fc=<span class="string">&quot;0.8&quot;</span>)  </span><br><span class="line">    numLeafs = getNumLeafs(myTree)  </span><br><span class="line">    depth = getTreeDepth(myTree)  </span><br><span class="line">    firstStr = <span class="built_in">next</span>(<span class="built_in">iter</span>(myTree))  </span><br><span class="line">    cntrPt = (plotTree.x0ff + (<span class="number">1.0</span> + <span class="built_in">float</span>(numLeafs))/<span class="number">2.0</span>/plotTree.totalW,plotTree.y0ff)  </span><br><span class="line">    plotMidText(cntrPt,parentPt,nodeTxt)  </span><br><span class="line">    plotNode(firstStr,cntrPt,parentPt,decisionNode)  </span><br><span class="line">    secondDict = myTree[firstStr]  </span><br><span class="line">    plotTree.y0ff = plotTree.y0ff - <span class="number">1.0</span>/plotTree.totalD  </span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():  </span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">type</span>(secondDict[key]).__name__==<span class="string">&#x27;dict&#x27;</span>:  </span><br><span class="line">            plotTree(secondDict[key],cntrPt,<span class="built_in">str</span>(key))  </span><br><span class="line">        <span class="keyword">else</span>:  </span><br><span class="line">            plotTree.x0ff = plotTree.x0ff + <span class="number">1.0</span>/plotTree.totalW  </span><br><span class="line">            plotNode(secondDict[key],(plotTree.x0ff,plotTree.y0ff),cntrPt,leafNode)  </span><br><span class="line">            plotMidText((plotTree.x0ff,plotTree.y0ff),cntrPt,<span class="built_in">str</span>(key))  </span><br><span class="line">    plotTree.y0ff = plotTree.y0ff + <span class="number">1.0</span>/plotTree.totalD  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">def</span> <span class="title function_">createPlot</span>(<span class="params">inTree</span>):  </span><br><span class="line">    fig = plt.figure(<span class="number">1</span>,facecolor=<span class="string">&#x27;white&#x27;</span>)  </span><br><span class="line">    fig.clf()  </span><br><span class="line">    axprops = <span class="built_in">dict</span>(xticks=[],yticks=[])  </span><br><span class="line">    createPlot.ax1 = plt.subplot(<span class="number">111</span>,frameon=<span class="literal">False</span>,**axprops)  </span><br><span class="line">    plotTree.totalW = <span class="built_in">float</span>(getNumLeafs(inTree))  </span><br><span class="line">    plotTree.totalD = <span class="built_in">float</span>(getTreeDepth(inTree))  </span><br><span class="line">    plotTree.x0ff = -<span class="number">0.5</span>/plotTree.totalW;plotTree.y0ff = <span class="number">1.0</span>;  </span><br><span class="line">    plotTree(inTree,(<span class="number">0.5</span>,<span class="number">1.0</span>), <span class="string">&#x27;&#x27;</span>)  </span><br><span class="line">    plt.show()  </span><br><span class="line">  </span><br><span class="line"><span class="comment">#分类  </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">classify</span>(<span class="params">inputTree,featLabels,testVec</span>):  </span><br><span class="line">    firstStr = <span class="built_in">next</span>(<span class="built_in">iter</span>(inputTree))  </span><br><span class="line">    secondDict = inputTree[firstStr]  </span><br><span class="line">    featIndex = featLabels.index(firstStr)  </span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> secondDict.keys():  </span><br><span class="line">        <span class="keyword">if</span> testVec[featIndex] == key:  </span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">type</span>(secondDict[key]).__name__ == <span class="string">&#x27;dict&#x27;</span>:  </span><br><span class="line">                classLabel = classify(secondDict[key],featLabels,testVec)  </span><br><span class="line">            <span class="keyword">else</span>:  </span><br><span class="line">                classLabel = secondDict[key]  </span><br><span class="line">    <span class="keyword">return</span> classLabel  </span><br><span class="line">  </span><br><span class="line"><span class="comment">#存储  </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">storeTree</span>(<span class="params">inputTree,filename</span>):  </span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filename,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fw:  </span><br><span class="line">        pickle.dump(inputTree,fw)  </span><br><span class="line">  </span><br><span class="line"><span class="comment">#载入  </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">grabTree</span>(<span class="params">filename</span>):  </span><br><span class="line">    fr = <span class="built_in">open</span>(filename,<span class="string">&#x27;rb&#x27;</span>)  </span><br><span class="line">    <span class="keyword">return</span> pickle.load(fr)  </span><br><span class="line">  </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:  </span><br><span class="line">    dataSet,labels = createDataSet()  </span><br><span class="line">    <span class="built_in">print</span>(dataSet)  </span><br><span class="line">    <span class="built_in">print</span>(labels)  </span><br><span class="line">    <span class="built_in">print</span>(calcShannonEnt(dataSet))  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;最优特征索引值：&quot;</span> + <span class="built_in">str</span>(chooseBestFeatureToSplit(dataSet)))  </span><br><span class="line">    featLabels = []  </span><br><span class="line">    myTree = createTree(dataSet,labels,featLabels)  </span><br><span class="line">    <span class="built_in">print</span>(myTree)  </span><br><span class="line">    createPlot(myTree)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">#测试  </span></span><br><span class="line">    test = [<span class="number">0</span>,<span class="number">0</span>]  </span><br><span class="line">    result = classify(myTree,featLabels,test)  </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n测试0，0的决策树结果:&quot;</span>)  </span><br><span class="line">    <span class="keyword">if</span> result == <span class="string">&#x27;yes&#x27;</span>:  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;发放贷款&quot;</span>)  </span><br><span class="line">    <span class="keyword">else</span>:  </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;不予贷款&quot;</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">#存储决策树  </span></span><br><span class="line">    storeTree(myTree,<span class="string">&#x27;classifierStore.txt&#x27;</span>)  </span><br><span class="line">  </span><br><span class="line">    <span class="comment">#载入决策树  </span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n载入决策树：&quot;</span>)  </span><br><span class="line">    loadMyTree = grabTree(<span class="string">&#x27;classifierStore.txt&#x27;</span>)  </span><br><span class="line">    <span class="built_in">print</span>(loadMyTree)  </span><br></pre></td></tr></table></figure><p><strong>运行代码截图</strong></p><p><img src="/../images/postsImg/%E5%86%B3%E7%AD%96%E6%A0%91%E5%AE%9E%E9%AA%8C/image-20241209152040685.png" alt="image-20241209152040685"></p><h2 id="六、实验结论"><a href="#六、实验结论" class="headerlink" title="六、实验结论"></a>六、实验结论</h2><p>​ID3算法使用信息增益进行特征选择，集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差。特征所对应的信息增益值最大，该特征就为最优特征，也就是说信息增益越大，越应该放在决策树的上层。</p><p>​利用递归方法建树，每次对象为当前最优特征。</p><p>​递归函数的第一个停止条件是所有的类标签都相同，递归函数第二个停止条件是使用完数据集中所有的特征，即数据集不能继续划分；字典变量myTree储存了树的所有信息，bestFeature则是当前最优特征。</p>]]></content>
      
      
      <categories>
          
          <category> 学习日常 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 作业 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习论文阅读报告</title>
      <link href="/2024/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/"/>
      <url>/2024/05/08/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/</url>
      
        <content type="html"><![CDATA[<h1 id="深度学习论文阅读报告"><a href="#深度学习论文阅读报告" class="headerlink" title="深度学习论文阅读报告"></a>深度学习论文阅读报告</h1><h2 id="一、论文背景与意义"><a href="#一、论文背景与意义" class="headerlink" title="一、论文背景与意义"></a>一、论文背景与意义</h2><h3 id="1-1论文简介"><a href="#1-1论文简介" class="headerlink" title="1.1论文简介"></a>1.1论文简介</h3><p>名称：基于 Bert-BiLSTM混合模型的社交媒体虚假信息识别研究</p><p>作者：冯由玲，康鑫，周金娉，李军</p><p>来源：《情报科学》期刊2024-01-29</p><h3 id="1-2论文背景与重要性"><a href="#1-2论文背景与重要性" class="headerlink" title="1.2论文背景与重要性"></a>1.2论文背景与重要性</h3><p><strong>论文背景</strong></p><p>​当今社会背景下随着互联网的深度普及和技术的不断提升，社交媒体中的真伪信息鱼龙混杂，而虚假信息一旦被接受，便很难被更正，这将对公众认知产生严重负面影响。因此，虚假信息也被世界经济论坛列为对未来社会的主要威胁研究社交媒体平台评论信息特征及真伪识别问题迫在眉睫。</p><p><strong>论文主题</strong></p><p>​Twitter平台中疫情主题相关推文的虚假信息识别研究</p><p><strong>论文研究的重要性</strong></p><ul><li>提高信息识别的准确性</li><li>加速信息处理速度</li><li>促进社会舆论的监控</li><li>推动深度学习在信息安全领域的应用</li></ul><h3 id="1-3论文结构"><a href="#1-3论文结构" class="headerlink" title="1.3论文结构"></a>1.3论文结构</h3><p><img src="/../images/postsImg/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/%E8%AE%BA%E6%96%87%E7%BB%93%E6%9E%84.png" alt="论文结构"></p><h2 id="二、论文分析"><a href="#二、论文分析" class="headerlink" title="二、论文分析"></a>二、论文分析</h2><h3 id="2-1相关研究"><a href="#2-1相关研究" class="headerlink" title="2.1相关研究"></a>2.1相关研究</h3><p>​虚假信息识别是一种通过构建模型的文本分类任务，通过对不同标签的文本内容进行特征提取，将文本分为虚假信息类和非虚假信息类。目前，学者们针对虚假信息识别的手段主要有三种：</p><ol><li>基于传统方法的文本特征识别：费时费力，覆盖面窄。</li><li>基于机器学习算法的识别：对数据特征要求高，人工处理费时费力，普适性较弱。</li><li>基于深度学习算法的识别：Bert预训练，融合双向长短时记忆网络算法(BiLSTM)，构建模型Bert-BiLSTM混合模型，识别虚假疫情信息。</li></ol><h3 id="2-2理论研究设计"><a href="#2-2理论研究设计" class="headerlink" title="2.2理论研究设计"></a>2.2理论研究设计</h3><p>​LDA主题词模型：分析文本中隐含主题和关键信息</p><p><img src="/../images/postsImg/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/image-20241210102201168.png" alt="image-20241210102201168"></p><p>​Bert预处理方法：捕捉文本间的语义关系</p><p><img src="/../images/postsImg/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/image-20241210102227536.png" alt="image-20241210102227536"></p><p>​BiLSTM算法：以前向和后向两个方向处理数据，捕获序列数据中的动态时间关系</p><p><img src="/../images/postsImg/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/image-20241210102239988.png" alt="image-20241210102239988"></p><h3 id="2-3LDA特征分析"><a href="#2-3LDA特征分析" class="headerlink" title="2.3LDA特征分析"></a>2.3LDA特征分析</h3><p>​文本长度分析：真实信息比虚假信息的文本长度更长</p><p><img src="/../images/postsImg/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/image-20241210102400259.png" alt="image-20241210102400259"></p><p>​数据情感分析：虚假信息表述方式相对积极，对疫情信息的整体情感比真实信息更加乐观。 虚假信息相对真实信息情感得分较高。</p><p><img src="/../images/postsImg/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/image-20241210102416484.png" alt="image-20241210102416484"></p><p>​将数据使用正则表达式过滤虚词，通过NLTK对数据进行词干提取，得到归一化的文本数据，分别提取其中真实数据和虚假数据。</p><p>​分别基于真实信息的LDA模型构建和基于虚假信息的LDA模型构建,分析出两者在内容特征、表述特征和主题特征三个维度的主要差别：</p><p><img src="/../images/postsImg/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/image-20241210102515468.png" alt="image-20241210102515468"></p><h3 id="2-4Bert-BiLSTM模型实验"><a href="#2-4Bert-BiLSTM模型实验" class="headerlink" title="2.4Bert-BiLSTM模型实验"></a>2.4Bert-BiLSTM模型实验</h3><ol><li>机器学习模型构建:运用 TF-IDF 和 Bert 将文本数据进行向量化处理后，分别引入到SVM（支持向量机）和RF（随机森林）。</li><li>深度学习模型构建:采用Keras对文本进行预处理，添加BiLSTM层（双向长短时记忆网络）和全连接层，选取激活函数Sigmoid对模型进行二分类。</li><li>Bert模型参数优化:选取Bert作为预训练模型，参数Batch size设置为128，最大长度设置为512，并开启了最大填充以契合深度学习模型。</li></ol><h3 id="2-5结果分析"><a href="#2-5结果分析" class="headerlink" title="2.5结果分析"></a>2.5结果分析</h3><p>​评价指标：准确率、精确率、召回率、F1值</p><p>​准确率评估预测正确的比例；精确率评估预测正例的查准率；召回率评估真实正例的查全率；F1&#x3D;（2<em>精确率</em>召回率）&#x2F; （精确率+召回率）</p><p>​<strong>基于传统编码算法的模型评估</strong></p><p><img src="/../images/postsImg/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/image-20241210102725770.png" alt="image-20241210102725770"></p><p>​支持向量机SVM作为传统机器学习算法表现较差，集成机器学习算法随机森林RF表现较好，深度学习模型 BiLSTM 表现最佳，准确率高达 95.7%，这反映了 BiLSTM 在处理较长文本数据时识别效果更好。</p><p>​<strong>融合Bert预处理的模型评估</strong></p><p><img src="/../images/postsImg/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E6%8A%A5%E5%91%8A/image-20241210102747593.png" alt="image-20241210102747593"></p><p>​对于 Bert 预处理后的数据， BiLSTM 有最好的表现并且优于传统预处理方式下的三种模型。</p><p>​总的来说，与传统预处理方式相比，Bert-BiLSTM 模型在精确率上提高了 1%。</p><h2 id="三、见解与启示"><a href="#三、见解与启示" class="headerlink" title="三、见解与启示"></a>三、见解与启示</h2><p><strong>优势</strong></p><ul><li>多主题低成本：对 Bert 模型进行了迁移构建了混合模型，相对于现有虚假疫情信息识别方法，该模型能对多主题英文文本进行低成本有效识别。</li><li>鲁棒性好：模型的健壮性较好，可以扩展到其他领域复杂主题虚假信息的研究中。</li><li>小规模数据集：运用 LDA 主题模型探究了疫情信息的特征，在小规模数据集上以较低成本实现了多主题数据的有效识别，为信息疫情治理提供了高效的解决方案。</li><li>对比试验：通过多方面对比使用，可靠性和真实性较高。</li></ul><p><strong>缺陷</strong></p><ul><li>数据集问题：当前使用的数据集长度较短，导致Bert模型需填充过多无效数据，影响性能。建议未来研究使用更长的文本数据。</li><li>仅限英文平台：中文社交媒体平台的虚假疫情信息数据量过小，本文仅针对英文社交媒体平台数据进行了分析。</li><li>算法测试限制：由于实验条件和数据限制，论文只测试了几种基本的机器学习模型。未来研究可以探索更多适合的算法，以优化虚假信息的识别效果。</li><li>数据忽略表情元素：在基于LDA进行疫情信息特征分析时，通过正则表达式清洗掉原始数据中的大量链接锚和表情符号，得到本文的初始数据。清洗掉表情符号会对结果造成一定影响，现代社会表情符号在语句中的作用也非常重要。</li></ul><h2 id="四、结论"><a href="#四、结论" class="headerlink" title="四、结论"></a>四、结论</h2><p><strong>价值</strong></p><ul><li>降低成本：该混合模型可以在资源受限环境进行精准识别，有利于降低识别成本</li><li>跨领域应用：研究成果不仅限于疫情信息，还可能扩展到其他类型的社交媒体虚假信息识别</li></ul><p><strong>意义</strong></p><p>​本文的研究成果对于减少社交媒体上虚假疫情信息的传播具有重要价值，有助于增强公众对健康信息的正确理解，同时对信息疫情治理提供了有效的技术解决方案。此外，研究还为类似问题提供了方法论的参考，具有广泛的应用前景和社会影响。</p>]]></content>
      
      
      <categories>
          
          <category> 学习日常 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 作业 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 论文 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>穷举搜索实验</title>
      <link href="/2024/05/06/%E7%A9%B7%E4%B8%BE%E6%90%9C%E7%B4%A2%E5%AE%9E%E9%AA%8C/"/>
      <url>/2024/05/06/%E7%A9%B7%E4%B8%BE%E6%90%9C%E7%B4%A2%E5%AE%9E%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<h1 id="实验三-穷举的魅力"><a href="#实验三-穷举的魅力" class="headerlink" title="实验三 穷举的魅力"></a>实验三 穷举的魅力</h1><h2 id="一、数据结构简介"><a href="#一、数据结构简介" class="headerlink" title="一、数据结构简介"></a>一、数据结构简介</h2><ol><li><p>树</p><p>​树有多个节点，用以存储元素。某些节点之间存在一定的关系，可用连线表示。</p><p><img src="/../images/postsImg/%E7%A9%B7%E4%B8%BE%E6%90%9C%E7%B4%A2%E5%AE%9E%E9%AA%8C/image-20241210091024345.png" alt="image-20241210091024345"></p></li><li><p>图</p><p>​图包括有向图和无向图，在此涉及的是无向连通图，在无向图中，顶点之间的边是没有方向的。</p><p><img src="/../images/postsImg/%E7%A9%B7%E4%B8%BE%E6%90%9C%E7%B4%A2%E5%AE%9E%E9%AA%8C/image-20241210091120844.png" alt="image-20241210091120844"></p></li><li><p>栈</p><pre><code>   栈的特点是插入和删除操作只能在一端进行，他按照先进后出的原则存储数据。</code></pre><p><img src="/../images/postsImg/%E7%A9%B7%E4%B8%BE%E6%90%9C%E7%B4%A2%E5%AE%9E%E9%AA%8C/image-20241210091132541.png" alt="image-20241210091132541"></p></li><li><p>队列</p><p>​    优先队列的区别是队列元素被赋予了优先级，插入元素未必直接插入队尾，而是按照优先级进行存取，删除元素时也是优先级最高的元素被删除。</p><p>​    <img src="/../images/postsImg/%E7%A9%B7%E4%B8%BE%E6%90%9C%E7%B4%A2%E5%AE%9E%E9%AA%8C/image-20241210091144620.png" alt="image-20241210091144620"></p></li></ol><h2 id="二、搜索算法"><a href="#二、搜索算法" class="headerlink" title="二、搜索算法"></a>二、搜索算法</h2><ol><li><p>搜索算法–DFS</p><p> DFS算法常用于对树或者图进行遍历。深度优先搜索会尽可能深得搜索树或者图的分支直到树或者图的所有节点均被访问。</p></li><li><p>搜索算法–BFS</p><p> BFS算法常用于对树或者图进行遍历。广度优先搜索会尽可能宽得搜索树或者图的分支直到树或者图的所有节点均被访问。</p><p>​</p></li></ol><h2 id="三、七桥问题"><a href="#三、七桥问题" class="headerlink" title="三、七桥问题"></a>三、七桥问题</h2><p>​河上有两个小岛，有七座桥把两个岛以及河岸联系起来，一个步行者如何不重复的，不遗漏的一次走完所有桥并且最终回到出发点。  </p><p><img src="/../images/postsImg/%E7%A9%B7%E4%B8%BE%E6%90%9C%E7%B4%A2%E5%AE%9E%E9%AA%8C/image-20241210091340866.png" alt="image-20241210091340866"></p><p><strong>七桥问题解法</strong></p><p>首先我们将七桥问题转换为一个图问题。</p><p><img src="/../images/postsImg/%E7%A9%B7%E4%B8%BE%E6%90%9C%E7%B4%A2%E5%AE%9E%E9%AA%8C/image-20241210091404810.png" alt="image-20241210091404810"></p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#七桥问题:</span></span><br><span class="line">vertex = [<span class="string">&#x27;A&#x27;</span>, <span class="string">&#x27;B&#x27;</span>, <span class="string">&#x27;C&#x27;</span>, <span class="string">&#x27;D&#x27;</span>]</span><br><span class="line">v2edge = [[<span class="string">&#x27;c&#x27;</span>,<span class="string">&#x27;f&#x27;</span>,<span class="string">&#x27;g&#x27;</span>],[<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;d&#x27;</span>,<span class="string">&#x27;e&#x27;</span>,<span class="string">&#x27;g&#x27;</span>],[<span class="string">&#x27;d&#x27;</span>,<span class="string">&#x27;e&#x27;</span>,<span class="string">&#x27;f&#x27;</span>],[<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>]]</span><br><span class="line">v2v = [[<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>],[<span class="number">3</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dfs_bridge</span>(<span class="params">vs,edg_num,v_path,edge_path</span>):</span><br><span class="line">    v_top = v_path[-<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> v_top == vs <span class="keyword">and</span> edg_num == <span class="built_in">len</span>(edge_path):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;起点是：&quot;</span>,vertex[vs])</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;依次通过的桥为：&quot;</span>,edge_path)</span><br><span class="line">    out_degree = <span class="built_in">len</span>(v2edge[v_top])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(out_degree):</span><br><span class="line">        <span class="keyword">if</span> v2edge[v_top][i] <span class="keyword">in</span> edge_path:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        v_path.append(v2v[v_top][i])</span><br><span class="line">        edge_path.append(v2edge[v_top][i])</span><br><span class="line">        dfs_bridge(vs,edg_num,v_path,edge_path)</span><br><span class="line">        v_path.pop()</span><br><span class="line">        edge_path.pop()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(vertex)):</span><br><span class="line">    dfs_bridge(vs=i,edg_num=<span class="number">7</span>,v_path=[i],edge_path=[])</span><br></pre></td></tr></table></figure><h2 id="四、旅行商问题"><a href="#四、旅行商问题" class="headerlink" title="四、旅行商问题"></a>四、旅行商问题</h2><p>​    选择一条旅行路径，该路径的限制是每个城市只能去一次并且不能漏掉某个城市，最后回到原点。</p><p><img src="/../images/postsImg/%E7%A9%B7%E4%B8%BE%E6%90%9C%E7%B4%A2%E5%AE%9E%E9%AA%8C/image-20241210091745259.png" alt="image-20241210091745259"></p><p><strong>七桥问题解法</strong></p><p>将城市看作顶点，将城市之间的路径看成边。 </p><p><img src="/../images/postsImg/%E7%A9%B7%E4%B8%BE%E6%90%9C%E7%B4%A2%E5%AE%9E%E9%AA%8C/image-20241210091824917.png" alt="image-20241210091824917"></p><p>代码实现如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#旅行社问题：</span></span><br><span class="line">vertex = [<span class="string">&#x27;A&#x27;</span>,<span class="string">&#x27;B&#x27;</span>,<span class="string">&#x27;C&#x27;</span>,<span class="string">&#x27;D&#x27;</span>,<span class="string">&#x27;S&#x27;</span>]</span><br><span class="line">v2edge = [[<span class="number">10</span>,<span class="number">8</span>],[<span class="number">10</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">3</span>],[<span class="number">8</span>,<span class="number">4</span>,<span class="number">2</span>,<span class="number">7</span>],[<span class="number">5</span>,<span class="number">2</span>,<span class="number">10</span>],[<span class="number">3</span>,<span class="number">7</span>,<span class="number">10</span>]]</span><br><span class="line">v2v = [[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">0</span>,<span class="number">1</span>,<span class="number">3</span>,<span class="number">4</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>],[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]]</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dfs_TSP</span>(<span class="params">vs,v_num,v_path,dis,min_dis</span>):</span><br><span class="line">    v_top = v_path[-<span class="number">1</span>]</span><br><span class="line">    out_degree = <span class="built_in">len</span>(v2edge[v_top])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(out_degree):</span><br><span class="line">        vs_i = v2v[v_top][i]</span><br><span class="line">        <span class="keyword">if</span> vs_i == vs:</span><br><span class="line">            <span class="keyword">if</span> v_num == <span class="built_in">len</span>(v_path) <span class="keyword">and</span> dis + v2edge[v_top][i] &lt; min_dis[<span class="number">0</span>]:</span><br><span class="line">                min_dis[<span class="number">0</span>] = dis + v2edge[v_top][i]</span><br><span class="line">                new_path = []</span><br><span class="line">                <span class="keyword">for</span> item <span class="keyword">in</span> v_path:</span><br><span class="line">                    new_path.append(vertex[item])</span><br><span class="line">                new_path.append(vertex[vs_i])</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;new distance:&quot;</span>,min_dis[<span class="number">0</span>],<span class="string">&quot;new path:&quot;</span>,new_path)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">if</span> vs_i <span class="keyword">in</span> v_path:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        v_path.append(v2v[v_top][i])</span><br><span class="line">        dfs_TSP(vs,v_num,v_path,dis + v2edge[v_top][i],min_dis)</span><br><span class="line">        v_path.pop()</span><br><span class="line">min_dis = [<span class="number">10000</span>]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(vertex)):</span><br><span class="line">    dfs_TSP(vs=i,v_num=<span class="number">5</span>,v_path=[i],dis=<span class="number">0</span>,min_dis=min_dis)</span><br></pre></td></tr></table></figure><h2 id="五、实验结论"><a href="#五、实验结论" class="headerlink" title="五、实验结论"></a>五、实验结论</h2><p><strong>穷举搜索法的优点</strong></p><ul><li>直观易懂：穷举搜索法通常基于问题的直接描述，易于理解和实现。</li><li>正确性保证：由于穷举搜索法会检查所有可能的解，因此它能够确保找到问题的最优解（如果存在的话）。</li><li>适用性广泛：这种方法适用于许多问题，特别是那些没有更简单或更有效算法可用的问题。</li></ul><p><strong>穷举搜索法的缺点</strong></p><ul><li>计算量大：穷举搜索法需要检查所有可能的解，因此计算量通常很大，尤其是在问题规模较大时。</li><li>效率低下：由于需要检查大量解，穷举搜索法通常非常耗时，可能在现实应用中不可行。</li><li>资源消耗多：在处理大规模问题时，穷举搜索法可能需要大量的内存和计算资源。</li></ul><p><strong>使用穷举搜索法需要注意的问题</strong></p><ul><li>问题规模：在使用穷举搜索法之前，需要仔细评估问题的规模。如果问题规模太大，穷举搜索法可能不是一个好选择。</li><li>优化技巧：尽管穷举搜索法本身可能效率低下，但可以通过一些优化技巧（如剪枝、排序、哈希等）来减少计算量和提高效率。</li><li>利用问题特性：在某些情况下，可以利用问题的特性（如对称性、单调性等）来减少搜索空间或优化搜索过程。</li></ul>]]></content>
      
      
      <categories>
          
          <category> 学习日常 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 作业 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>搜索算法实验</title>
      <link href="/2024/04/22/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E5%AE%9E%E9%AA%8C/"/>
      <url>/2024/04/22/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E5%AE%9E%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<h1 id="实验二-搜索算法实验"><a href="#实验二-搜索算法实验" class="headerlink" title="实验二 搜索算法实验"></a><strong>实验二 搜索算法实验</strong></h1><p>​对下面这个有向图进行广度优先搜索和深度优先搜索</p><p><img src="/../images/postsImg/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E5%AE%9E%E9%AA%8C/image-20241209143655870.png" alt="image-20241209143655870"></p><p>​用字典来表示整个图，图由多个节点组成。将节点与其所有邻近节点的关系用键值对来表示，代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">graph = &#123;&#125;</span><br><span class="line">graph[<span class="string">&quot;A&quot;</span>] = [<span class="string">&quot;B&quot;</span>, <span class="string">&quot;D&quot;</span>, <span class="string">&quot;F&quot;</span>]</span><br><span class="line">graph[<span class="string">&quot;B&quot;</span>] = [<span class="string">&quot;C&quot;</span>, <span class="string">&quot;E&quot;</span>]</span><br><span class="line">graph[<span class="string">&quot;D&quot;</span>] = [<span class="string">&quot;C&quot;</span>]</span><br><span class="line">graph[<span class="string">&quot;F&quot;</span>] = [<span class="string">&quot;G&quot;</span>, <span class="string">&quot;H&quot;</span>]</span><br><span class="line">graph[<span class="string">&quot;C&quot;</span>] = []</span><br><span class="line">graph[<span class="string">&quot;E&quot;</span>] = []</span><br><span class="line">graph[<span class="string">&quot;G&quot;</span>] = []</span><br><span class="line">graph[<span class="string">&quot;H&quot;</span>] = []</span><br></pre></td></tr></table></figure><p>​键表示每个节点，值是一个数组，其中包含了键的所有邻近节点，这里的邻近节点是指从键所表示的节点出发，箭头所指向的其他节点。没有邻近节点的键所对应的值为空列表。</p><h2 id="一、广度优先搜索"><a href="#一、广度优先搜索" class="headerlink" title="一、广度优先搜索"></a><strong>一、广度优先搜索</strong></h2><p>​使用函数deque来创建一个双端列表，可以实现在列表两端添加(append)和弹出(pop)元素。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line">search_queue = deque()     <span class="comment">#创建一个节点列表</span></span><br><span class="line">search_queue += graph[<span class="string">&quot;A&quot;</span>]     <span class="comment">#表示将&quot;A&quot;的相邻节点都添加到节点列表中</span></span><br></pre></td></tr></table></figure><p>•我们指定 “A” 为起点，”G” 为终点。</p><p>•需要一个 searched 数组来保存搜索过的节点，防止同一个节点被多次搜索。</p><p>•每次从节点列表的最左边取出节点。</p><p><strong>代码运行结果</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#运行上述代码，可以得到输出：</span></span><br><span class="line">B D F C E C G </span><br><span class="line">find the destination!</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><p>​根据节点的输出顺序就可以知道搜索顺序符合广度优先搜索的规则。</p><h2 id="二、深度优先搜索"><a href="#二、深度优先搜索" class="headerlink" title="二、深度优先搜索"></a><strong>二、深度优先搜索</strong></h2><p>​只需将上面代码中的</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">node = search_queue.popleft() <span class="comment"># 取出节点列表中最左边的节点</span></span><br><span class="line"><span class="comment">#改为如下语句</span></span><br><span class="line">node = search_queue.pop() <span class="comment"># 取出节点列表中最右边的节点</span></span><br></pre></td></tr></table></figure><p>​因为广度优先搜索和深度优先搜索的区别就只是选择哪一个候补节点作为下一个节点的基准不同。</p><p><strong>代码运行结果</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#运行上述代码，可以得到输出：</span></span><br><span class="line">F H G</span><br><span class="line">find the destination!</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure><p>​根据节点的输出顺序就可以知道搜索顺序符合深度优先搜索的规则。</p><h2 id="三、实验作业"><a href="#三、实验作业" class="headerlink" title="三、实验作业"></a><strong>三、实验作业</strong></h2><p>​代码实现使用深度优先搜索和广度优先搜索分别找到从A到I的路径</p><p><img src="/../images/postsImg/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E5%AE%9E%E9%AA%8C/image-20241209144243070.png" alt="image-20241209144243070"></p><h2 id="四、深度优先代码"><a href="#四、深度优先代码" class="headerlink" title="四、深度优先代码"></a>四、深度优先代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个图</span></span><br><span class="line">graph = &#123;&#125;</span><br><span class="line">graph[<span class="string">&quot;I&quot;</span>] = []</span><br><span class="line">graph[<span class="string">&quot;H&quot;</span>] = [<span class="string">&quot;I&quot;</span>]</span><br><span class="line">graph[<span class="string">&quot;G&quot;</span>] = [<span class="string">&quot;I&quot;</span>]</span><br><span class="line">graph[<span class="string">&quot;F&quot;</span>] = [<span class="string">&quot;E&quot;</span>, <span class="string">&quot;H&quot;</span>]</span><br><span class="line">graph[<span class="string">&quot;E&quot;</span>] = [<span class="string">&quot;H&quot;</span>]</span><br><span class="line">graph[<span class="string">&quot;D&quot;</span>] = [<span class="string">&quot;E&quot;</span>]</span><br><span class="line">graph[<span class="string">&quot;C&quot;</span>] = [<span class="string">&quot;G&quot;</span>, <span class="string">&quot;H&quot;</span>]</span><br><span class="line">graph[<span class="string">&quot;B&quot;</span>] = [<span class="string">&quot;F&quot;</span>]</span><br><span class="line">graph[<span class="string">&quot;A&quot;</span>] = [<span class="string">&quot;B&quot;</span>, <span class="string">&quot;C&quot;</span>, <span class="string">&quot;D&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#深度优先搜索</span></span><br><span class="line">search_queue = deque()</span><br><span class="line">search_queue += graph[<span class="string">&quot;A&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 搜索函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">start_node</span>):</span><br><span class="line">    search_queue = deque()</span><br><span class="line">    search_queue += graph[start_node]</span><br><span class="line">    searched = []</span><br><span class="line">    <span class="keyword">while</span> search_queue:</span><br><span class="line">        node = search_queue.pop()</span><br><span class="line">        <span class="built_in">print</span>(node,end=<span class="string">&quot; &quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> node <span class="keyword">in</span> searched:</span><br><span class="line">            <span class="keyword">if</span> node == <span class="string">&quot;I&quot;</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;\nfind the destination&quot;</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                search_queue += graph[node]</span><br><span class="line">                searched.append(node)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"><span class="built_in">print</span>(search(<span class="string">&quot;A&quot;</span>))</span><br></pre></td></tr></table></figure><p><strong>运行结果截图</strong></p><p><img src="/../images/postsImg/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E5%AE%9E%E9%AA%8C/image-20241209144503995.png" alt="image-20241209144503995"></p><h2 id="五、广度优先代码"><a href="#五、广度优先代码" class="headerlink" title="五、广度优先代码"></a>五、广度优先代码</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#广度优先搜索</span></span><br><span class="line">search_queue = deque()</span><br><span class="line">search_queue += graph[<span class="string">&quot;A&quot;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">start_node</span>):</span><br><span class="line">    search_queue = deque()</span><br><span class="line">    search_queue += graph[start_node]</span><br><span class="line">    searched = []</span><br><span class="line">    <span class="keyword">while</span> search_queue:</span><br><span class="line">        node = search_queue.popleft()</span><br><span class="line">        <span class="built_in">print</span>(node,end=<span class="string">&quot; &quot;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> node <span class="keyword">in</span> searched:</span><br><span class="line">            <span class="keyword">if</span> node == <span class="string">&quot;I&quot;</span>:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">&quot;\nfind the destination&quot;</span>)</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                search_queue += graph[node]</span><br><span class="line">                searched.append(node)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(search(<span class="string">&quot;A&quot;</span>))</span><br></pre></td></tr></table></figure><p><strong>运行结果截图</strong></p><p><img src="/../images/postsImg/%E6%90%9C%E7%B4%A2%E7%AE%97%E6%B3%95%E5%AE%9E%E9%AA%8C/image-20241209144609968.png" alt="image-20241209144609968"></p><h2 id="六、实验结论"><a href="#六、实验结论" class="headerlink" title="六、实验结论"></a>六、实验结论</h2><p>​本次实验使用了collections.deque。</p><p>​collections.deque: 是一个双端队列，它允许我们在两端进行插入和删除操作。这在广度优先搜索中非常有用，因为它允许我们按照进入队列的顺序访问节点。deque的构造函数只有一个参数，即一个可迭代对象，如列表、元组等。</p>]]></content>
      
      
      <categories>
          
          <category> 学习日常 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 作业 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>线性回归实验</title>
      <link href="/2024/03/12/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E9%AA%8C/"/>
      <url>/2024/03/12/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<h1 id="实验一-一元线性回归"><a href="#实验一-一元线性回归" class="headerlink" title="实验一 一元线性回归"></a><strong>实验一 一元线性回归</strong></h1><h2 id="一、实验目的"><a href="#一、实验目的" class="headerlink" title="一、实验目的"></a>一、实验目的</h2><p>1.掌握最小二乘法的代码实现；</p><p>2.理解一元线性回归方程的收敛过程；</p><p>3.使用库函数实现一元线性回归。</p><h2 id="二、实验内容"><a href="#二、实验内容" class="headerlink" title="二、实验内容"></a>二、实验内容</h2><p><img src="/../images/postsImg/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E9%AA%8C/image-20241209142538941-17337255444221.png" alt="image-20241209142538941"></p><p>​针对表格中的数据，使用最小二乘法分析得到最佳线性回归方程</p><p><img src="/../images/postsImg/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E9%AA%8C/image-20241209142627925-17337255894242.png" alt="image-20241209142627925"></p><p>​并求得最终的损失函数</p><p><img src="/../images/postsImg/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E9%AA%8C/image-20241209142639614-17337256009253.png" alt="image-20241209142639614"></p><h2 id="三、实验材料与工具"><a href="#三、实验材料与工具" class="headerlink" title="三、实验材料与工具"></a>三、实验材料与工具</h2><p>1.电脑一台；</p><p>2.Pycharm；</p><h2 id="四、实验步骤"><a href="#四、实验步骤" class="headerlink" title="四、实验步骤"></a>四、实验步骤</h2><p>1.引入相应的工具包并设置中文字体</p><p><img src="/../images/postsImg/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E9%AA%8C/image-20241209142732029-17337256531894.png" alt="image-20241209142732029"></p><p>2.借助pandas库读取数据集</p><p><img src="/../images/postsImg/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E9%AA%8C/image-20241209142741616.png" alt="image-20241209142741616"></p><p>3.在坐标系中打印出散点图</p><p><img src="/../images/postsImg/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E9%AA%8C/image-20241209142815506.png" alt="image-20241209142815506"></p><p>4.计算损失函数</p><p><img src="/../images/postsImg/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E9%AA%8C/image-20241209142829897.png" alt="image-20241209142829897"></p><p><img src="/../images/postsImg/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E9%AA%8C/image-20241209142837662.png" alt="image-20241209142837662"></p><p><img src="/../images/postsImg/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E9%AA%8C/image-20241209142847043.png" alt="image-20241209142847043"></p><p><img src="/../images/postsImg/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E9%AA%8C/image-20241209142851061.png" alt="image-20241209142851061"></p><p><img src="/../images/postsImg/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E9%AA%8C/image-20241209142854764.png" alt="image-20241209142854764"></p><p>5.绘制回归线</p><p><img src="/../images/postsImg/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E9%AA%8C/image-20241209142903010.png" alt="image-20241209142903010"></p><p><img src="/../images/postsImg/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E9%AA%8C/image-20241209142905979.png" alt="image-20241209142905979"></p><p>6.使用sklearn库实现一元线性回归</p><p><img src="/../images/postsImg/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E9%AA%8C/image-20241209142914537.png" alt="image-20241209142914537"></p><p>7.调用库函数训练模型</p><p><img src="/../images/postsImg/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E9%AA%8C/image-20241209142920013.png" alt="image-20241209142920013"></p><p>8.输出回归方程并预测任意温度下火灾的受灾面积</p><p><img src="/../images/postsImg/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E9%AA%8C/image-20241209142928526.png" alt="image-20241209142928526"></p><p>完整代码：</p><p><img src="/../images/postsImg/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%AE%9E%E9%AA%8C/image-20241209142938979.png" alt="image-20241209142938979"></p><h2 id="五、实验结论"><a href="#五、实验结论" class="headerlink" title="五、实验结论"></a>五、实验结论</h2><p>​sklearn库实现一元线性回归</p><p>​LinearRegression()帮助我们根据给定的输入特征和目标变量拟合一个线性模型，以便进行预测和分析。</p><p>​fit_intercept（默认为True）</p><p>​fit_intercept参数控制是否在模型中包含截距（偏置）项。截距项用于解决数据中的偏差问题，确保回归线能够在目标变量不为零时仍具有适当的截距。当fit_intercept为True时，模型会自动添加截距项；当为False时，模型不会添加截距项。</p><p>​copy_X（默认为True）</p><p>​copy_X参数控制是否复制输入特征数据。当copy_X为True时，输入特征数据会被复制，以防止对原始数据的修改。当copy_X为False时，模型将直接使用传入的输入特征数据。</p><p>​n_jobs（默认为None）</p><p>​n_jobs参数指定模型拟合过程中要使用的CPU核心数量。如果设置为None，则使用默认的核心数。如果设置为-1，则使用所有可用的核心。</p>]]></content>
      
      
      <categories>
          
          <category> 学习日常 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 人工智能 </tag>
            
            <tag> 作业 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java基础知识</title>
      <link href="/2020/12/04/Java%E5%9F%BA%E7%A1%80/"/>
      <url>/2020/12/04/Java%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><h3 id="基本数据类型"><a href="#基本数据类型" class="headerlink" title="基本数据类型"></a>基本数据类型</h3><p>byte  占1字节  </p><p>short  占2字节</p><p>int  占4字节</p><p>long  占8字节</p><table><thead><tr><th></th><th align="left">byte</th><th align="left">short</th><th align="left">int</th><th align="left">long</th><th><strong>float</strong></th><th><strong>double</strong></th><th><strong>char</strong></th><th><strong>boolean</strong></th></tr></thead><tbody><tr><td>大小</td><td align="left">8位</td><td align="left">16 位</td><td align="left">32位</td><td align="left">64 位</td><td>32位</td><td>64 位</td><td>16 位</td><td>1位</td></tr><tr><td></td><td align="left">最大存储数据量是255</td><td align="left">最大数据存储量是65536</td><td align="left">最大数据存储容量是2的32次方减1</td><td align="left">最大数据存储容量是2的64次方减1</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td align="left">数据范围是-128~127之间</td><td align="left">数据范围是-32768~32767之间</td><td align="left">数据范围是负的2的31次方到正的2的31次方减1</td><td align="left">数据范围为负的2的63次方到正的2的63次方减1</td><td>数据范围在3.4e-45~1.4e38</td><td>数据范围在4.9e-324~1.8e308</td><td>只有true和false两个取值</td><td>存储Unicode码，用单引号赋值</td></tr><tr><td>默认值</td><td align="left">0</td><td align="left">0</td><td align="left">0</td><td align="left"><strong>0L</strong></td><td><strong>0.0f</strong></td><td><strong>0.0d</strong></td><td></td><td><strong>false</strong></td></tr><tr><td></td><td align="left"></td><td align="left"></td><td align="left"></td><td align="left"></td><td></td><td></td><td></td><td></td></tr></tbody></table><table><thead><tr><th></th><th></th><th></th><th></th><th></th></tr></thead><tbody><tr><td>byte</td><td></td><td></td><td></td><td></td></tr><tr><td>short</td><td></td><td></td><td></td><td></td></tr><tr><td>int</td><td></td><td></td><td></td><td></td></tr><tr><td>long</td><td></td><td></td><td></td><td></td></tr><tr><td>float</td><td></td><td></td><td></td><td></td></tr><tr><td>double</td><td></td><td></td><td></td><td></td></tr><tr><td>char</td><td></td><td></td><td></td><td></td></tr><tr><td>boolean</td><td></td><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>位（bit）：是计算机 内部数据 储存的最小单位，11001100是一个八位二进制数。<br>字节（byte）：是计算机中 数据处理 的基本单位，习惯上用大写 B 来表示，<br>1B (byte,字节) &#x3D; 8bit (位)<br>字符：是指计算机中使用的字母、数字、字和符号</p><p>1bit表示1位,<br>1Byte表示<br>个字节1B&#x3D;8b。<br>1024B&#x3D;1KB<br>1024KB&#x3D;1M<br>1024M&#x3D;1G.</p><h3 id="引用数据类型"><a href="#引用数据类型" class="headerlink" title="引用数据类型"></a>引用数据类型</h3>]]></content>
      
      
      <categories>
          
          <category> Java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Typora </tag>
            
            <tag> 技术 </tag>
            
            <tag> Java </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
